{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"../json_data/quadrant_experiment_conversations.json\"\n",
    "OUTPUT_FILE = \"../json_data/line_experiment_conversations2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(OUTPUT_FILE, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "len(data)\n",
    "\n",
    "# # 2) Transform each list (conversation) into a dict with a 'conversations' key.\n",
    "# wrapped_data = []\n",
    "\n",
    "# for i, conversation_list in enumerate(data):\n",
    "#     if i % 50 == 0:\n",
    "#         print(f\"Processing conversation {i}...\")\n",
    "#     wrapped_data.append({\n",
    "#         \"conversations\": conversation_list\n",
    "#     })\n",
    "\n",
    "# # 3) Write the new JSON\n",
    "# with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(wrapped_data, f, ensure_ascii=False)\n",
    "\n",
    "# print(f\"Done! Wrote {len(wrapped_data)} items to {OUTPUT_FILE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-15 12:00:47,184] [WARNING] [real_accelerator.py:162:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-01-15 12:00:47,211] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, Optional, List\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from deepspeed import zero\n",
    "from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\n",
    "import transformers\n",
    "from transformers import Trainer, GPTQConfig, deepspeed\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "from torch.utils.data import DataLoader as Dataloader\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "\n",
    "def preprocess_sources(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    max_len: int,\n",
    "    system_message: str = \"You are a helpful assistant.\"\n",
    ") -> Dict:\n",
    "    roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\n",
    "\n",
    "    im_start = tokenizer.im_start_id\n",
    "    im_end = tokenizer.im_end_id\n",
    "    nl_tokens = tokenizer('\\n').input_ids\n",
    "    _system = tokenizer('system').input_ids + nl_tokens\n",
    "    _user = tokenizer('user').input_ids + nl_tokens\n",
    "    _assistant = tokenizer('assistant').input_ids + nl_tokens\n",
    "\n",
    "    # Apply prompt templates\n",
    "    input_ids, targets = [], []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != roles[\"user\"]:\n",
    "            source = source[1:]\n",
    "\n",
    "        input_id, target = [], []\n",
    "        system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n",
    "        input_id += system\n",
    "        target += [im_start] + [IGNORE_TOKEN_ID] * (len(system)-3) + [im_end] + nl_tokens\n",
    "        assert len(input_id) == len(target)\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            _input_id = tokenizer(role).input_ids + nl_tokens + \\\n",
    "                tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n",
    "            input_id += _input_id\n",
    "            if role == '<|im_start|>user':\n",
    "                _target = [im_start] + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + [im_end] + nl_tokens\n",
    "            elif role == '<|im_start|>assistant':\n",
    "                _target = [im_start] + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\n",
    "                    _input_id[len(tokenizer(role).input_ids)+1:-2] + [im_end] + nl_tokens\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            target += _target\n",
    "        assert len(input_id) == len(target)\n",
    "        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n",
    "        target += [IGNORE_TOKEN_ID] * (max_len - len(target))\n",
    "        input_ids.append(input_id[:max_len])\n",
    "        targets.append(target[:max_len])\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int)\n",
    "    targets = torch.tensor(targets, dtype=torch.int)\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, max_len: int):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        \n",
    "        sources = [example[\"conversations\"] for example in raw_data]\n",
    "        data_dict = preprocess_sources(sources, tokenizer, max_len)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        \"Qwen/Qwen-VL-Chat\",\n",
    "        model_max_length=4096,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "tokenizer.pad_token_id = tokenizer.eod_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SupervisedDataset(data, tokenizer, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
