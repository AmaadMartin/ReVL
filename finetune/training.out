wandb: Appending key for api.wandb.ai to your netrc file: /jet/home/amartin1/.netrc
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2024-09-18 21:03:03,663] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-18 21:03:03,665] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-18 21:03:03,670] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-18 21:03:03,673] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-18 21:03:03,675] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-18 21:03:03,680] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-18 21:03:03,687] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-18 21:03:03,690] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH

[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m NVIDIA Inference is only supported on Ampere and newer architectures
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
[2024-09-18 21:03:08,562] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-18 21:03:08,562] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-18 21:03:08,562] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-18 21:03:08,562] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-18 21:03:08,576] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-09-18 21:03:08,562] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-18 21:03:08,562] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-18 21:03:08,562] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-09-18 21:03:08,562] [INFO] [comm.py:637:init_distributed] cdb=None
model_args:model_args:model_args:   ModelArguments(model_name_or_path='Qwen/Qwen-VL-Chat')ModelArguments(model_name_or_path='Qwen/Qwen-VL-Chat')
ModelArguments(model_name_or_path='Qwen/Qwen-VL-Chat')
data_args:
data_args: data_args: DataArguments(data_path='data/json_data/small_seeclick_web_coordinate_train.json', eval_data_path=None, lazy_preprocess=True) DataArguments(data_path='data/json_data/small_seeclick_web_coordinate_train.json', eval_data_path=None, lazy_preprocess=True)
DataArguments(data_path='data/json_data/small_seeclick_web_coordinate_train.json', eval_data_path=None, lazy_preprocess=True)
training_args:
training_args: training_args:  model_args: ModelArguments(model_name_or_path='Qwen/Qwen-VL-Chat')
data_args: DataArguments(data_path='data/json_data/small_seeclick_web_coordinate_train.json', eval_data_path=None, lazy_preprocess=True)
training_args: TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.95,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=finetune/config/ds_config_zero2.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60,
evaluation_strategy=steps,
fix_vit=True,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=6,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_qwen/runs/Sep18_21-03-08_v009.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=adamw_torch,
optim_args=None,
output_dir=output_qwen,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=output_qwen,
save_on_each_node=False,
save_safetensors=False,
save_steps=60,
save_strategy=steps,
save_total_limit=10,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.01,
warmup_steps=0,
weight_decay=0.1,
)TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.95,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=finetune/config/ds_config_zero2.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60,
evaluation_strategy=steps,
fix_vit=True,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=4,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_qwen/runs/Sep18_21-03-08_v009.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=adamw_torch,
optim_args=None,
output_dir=output_qwen,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=output_qwen,
save_on_each_node=False,
save_safetensors=False,
save_steps=60,
save_strategy=steps,
save_total_limit=10,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.01,
warmup_steps=0,
weight_decay=0.1,
)TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.95,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=finetune/config/ds_config_zero2.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60,
evaluation_strategy=steps,
fix_vit=True,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=7,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_qwen/runs/Sep18_21-03-08_v009.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=adamw_torch,
optim_args=None,
output_dir=output_qwen,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=output_qwen,
save_on_each_node=False,
save_safetensors=False,
save_steps=60,
save_strategy=steps,
save_total_limit=10,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.01,
warmup_steps=0,
weight_decay=0.1,
)


lora_args:lora_args:lora_args:   LoraArguments(lora_r=64, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['c_attn', 'attn.c_proj', 'w1', 'w2'], lora_weight_path='', lora_bias='none', q_lora=False)LoraArguments(lora_r=64, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['c_attn', 'attn.c_proj', 'w1', 'w2'], lora_weight_path='', lora_bias='none', q_lora=False)
LoraArguments(lora_r=64, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['c_attn', 'attn.c_proj', 'w1', 'w2'], lora_weight_path='', lora_bias='none', q_lora=False)

TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.95,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=finetune/config/ds_config_zero2.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60,
evaluation_strategy=steps,
fix_vit=True,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_qwen/runs/Sep18_21-03-08_v009.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=adamw_torch,
optim_args=None,
output_dir=output_qwen,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=output_qwen,
save_on_each_node=False,
save_safetensors=False,
save_steps=60,
save_strategy=steps,
save_total_limit=10,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.01,
warmup_steps=0,
weight_decay=0.1,
)
lora_args: LoraArguments(lora_r=64, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['c_attn', 'attn.c_proj', 'w1', 'w2'], lora_weight_path='', lora_bias='none', q_lora=False)
model_args: ModelArguments(model_name_or_path='Qwen/Qwen-VL-Chat')
data_args: DataArguments(data_path='data/json_data/small_seeclick_web_coordinate_train.json', eval_data_path=None, lazy_preprocess=True)
training_args:model_args:  ModelArguments(model_name_or_path='Qwen/Qwen-VL-Chat')
data_args: DataArguments(data_path='data/json_data/small_seeclick_web_coordinate_train.json', eval_data_path=None, lazy_preprocess=True)
training_args: model_args: ModelArguments(model_name_or_path='Qwen/Qwen-VL-Chat')
data_args: DataArguments(data_path='data/json_data/small_seeclick_web_coordinate_train.json', eval_data_path=None, lazy_preprocess=True)
training_args: TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.95,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=finetune/config/ds_config_zero2.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60,
evaluation_strategy=steps,
fix_vit=True,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=3,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_qwen/runs/Sep18_21-03-08_v009.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=adamw_torch,
optim_args=None,
output_dir=output_qwen,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=output_qwen,
save_on_each_node=False,
save_safetensors=False,
save_steps=60,
save_strategy=steps,
save_total_limit=10,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.01,
warmup_steps=0,
weight_decay=0.1,
)
lora_args: LoraArguments(lora_r=64, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['c_attn', 'attn.c_proj', 'w1', 'w2'], lora_weight_path='', lora_bias='none', q_lora=False)TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.95,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=finetune/config/ds_config_zero2.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60,
evaluation_strategy=steps,
fix_vit=True,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=5,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_qwen/runs/Sep18_21-03-08_v009.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=adamw_torch,
optim_args=None,
output_dir=output_qwen,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=output_qwen,
save_on_each_node=False,
save_safetensors=False,
save_steps=60,
save_strategy=steps,
save_total_limit=10,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.01,
warmup_steps=0,
weight_decay=0.1,
)

lora_args: LoraArguments(lora_r=64, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['c_attn', 'attn.c_proj', 'w1', 'w2'], lora_weight_path='', lora_bias='none', q_lora=False)
TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.95,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=finetune/config/ds_config_zero2.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60,
evaluation_strategy=steps,
fix_vit=True,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_qwen/runs/Sep18_21-03-08_v009.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=adamw_torch,
optim_args=None,
output_dir=output_qwen,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=output_qwen,
save_on_each_node=False,
save_safetensors=False,
save_steps=60,
save_strategy=steps,
save_total_limit=10,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.01,
warmup_steps=0,
weight_decay=0.1,
)
lora_args: LoraArguments(lora_r=64, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['c_attn', 'attn.c_proj', 'w1', 'w2'], lora_weight_path='', lora_bias='none', q_lora=False)
model_args: ModelArguments(model_name_or_path='Qwen/Qwen-VL-Chat')
data_args: DataArguments(data_path='data/json_data/small_seeclick_web_coordinate_train.json', eval_data_path=None, lazy_preprocess=True)
training_args: TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.95,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
cache_dir=None,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=finetune/config/ds_config_zero2.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=60,
evaluation_strategy=steps,
fix_vit=True,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=8,
gradient_checkpointing=True,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_qwen/runs/Sep18_21-03-08_v009.ib.bridges2.psc.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=1.0,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
model_max_length=2048,
mp_parameters=,
no_cuda=False,
num_train_epochs=5.0,
optim=adamw_torch,
optim_args=None,
output_dir=output_qwen,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=output_qwen,
save_on_each_node=False,
save_safetensors=False,
save_steps=60,
save_strategy=steps,
save_total_limit=10,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_lora=True,
use_mps_device=False,
warmup_ratio=0.01,
warmup_steps=0,
weight_decay=0.1,
)
lora_args: LoraArguments(lora_r=64, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['c_attn', 'attn.c_proj', 'w1', 'w2'], lora_weight_path='', lora_bias='none', q_lora=False)
The model is automatically converting to fp16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to fp16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to fp16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to fp16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to fp16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to fp16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to fp16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
The model is automatically converting to fp16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to "AutoModelForCausalLM.from_pretrained".
Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:  10%|█         | 1/10 [00:09<01:21,  9.03s/it]Loading checkpoint shards:  10%|█         | 1/10 [00:11<01:43, 11.53s/it]Loading checkpoint shards:  10%|█         | 1/10 [00:10<01:33, 10.42s/it]Loading checkpoint shards:  10%|█         | 1/10 [00:10<01:38, 10.91s/it]Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:  10%|█         | 1/10 [00:02<00:18,  2.09s/it]Loading checkpoint shards:  10%|█         | 1/10 [00:02<00:19,  2.15s/it]Loading checkpoint shards:  10%|█         | 1/10 [00:02<00:18,  2.02s/it]Loading checkpoint shards:  20%|██        | 2/10 [00:30<02:10, 16.27s/it]Loading checkpoint shards:  20%|██        | 2/10 [00:32<02:18, 17.30s/it]Loading checkpoint shards:  20%|██        | 2/10 [00:32<02:16, 17.05s/it]Loading checkpoint shards:  20%|██        | 2/10 [00:31<02:14, 16.85s/it]Loading checkpoint shards:  20%|██        | 2/10 [00:07<00:34,  4.31s/it]Loading checkpoint shards:  20%|██        | 2/10 [00:09<00:43,  5.48s/it]Loading checkpoint shards:  20%|██        | 2/10 [00:09<00:44,  5.51s/it]Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]Loading checkpoint shards:  10%|█         | 1/10 [00:01<00:13,  1.53s/it]Loading checkpoint shards:  20%|██        | 2/10 [00:03<00:13,  1.65s/it]Loading checkpoint shards:  30%|███       | 3/10 [00:52<02:11, 18.85s/it]Loading checkpoint shards:  30%|███       | 3/10 [00:54<02:14, 19.28s/it]Loading checkpoint shards:  30%|███       | 3/10 [00:54<02:15, 19.42s/it]Loading checkpoint shards:  30%|███       | 3/10 [00:53<02:14, 19.17s/it]Loading checkpoint shards:  30%|███       | 3/10 [00:31<01:30, 12.88s/it]Loading checkpoint shards:  30%|███       | 3/10 [00:31<01:30, 12.86s/it]Loading checkpoint shards:  30%|███       | 3/10 [00:19<00:59,  8.48s/it]Loading checkpoint shards:  30%|███       | 3/10 [00:30<01:27, 12.43s/it]Loading checkpoint shards:  40%|████      | 4/10 [01:01<01:30, 15.11s/it]Loading checkpoint shards:  40%|████      | 4/10 [01:04<01:32, 15.44s/it]Loading checkpoint shards:  40%|████      | 4/10 [01:03<01:31, 15.29s/it]Loading checkpoint shards:  40%|████      | 4/10 [01:03<01:32, 15.36s/it]Loading checkpoint shards:  40%|████      | 4/10 [00:41<01:09, 11.51s/it]Loading checkpoint shards:  40%|████      | 4/10 [00:41<01:09, 11.50s/it]Loading checkpoint shards:  40%|████      | 4/10 [00:29<00:53,  8.85s/it]Loading checkpoint shards:  40%|████      | 4/10 [00:39<01:07, 11.24s/it]Loading checkpoint shards:  50%|█████     | 5/10 [01:16<01:11, 14.36s/it]Loading checkpoint shards:  50%|█████     | 5/10 [01:15<01:11, 14.27s/it]Loading checkpoint shards:  50%|█████     | 5/10 [01:14<01:10, 14.15s/it]Loading checkpoint shards:  50%|█████     | 5/10 [01:16<01:11, 14.32s/it]Loading checkpoint shards:  50%|█████     | 5/10 [00:53<00:59, 11.82s/it]Loading checkpoint shards:  50%|█████     | 5/10 [00:53<00:59, 11.83s/it]Loading checkpoint shards:  50%|█████     | 5/10 [00:51<00:58, 11.62s/it]Loading checkpoint shards:  50%|█████     | 5/10 [00:41<00:50, 10.14s/it]Loading checkpoint shards:  60%|██████    | 6/10 [01:23<00:47, 11.97s/it]Loading checkpoint shards:  60%|██████    | 6/10 [01:22<00:47, 11.90s/it]Loading checkpoint shards:  60%|██████    | 6/10 [01:23<00:47, 11.93s/it]Loading checkpoint shards:  60%|██████    | 6/10 [01:21<00:47, 11.83s/it]Loading checkpoint shards:  60%|██████    | 6/10 [01:00<00:41, 10.32s/it]Loading checkpoint shards:  60%|██████    | 6/10 [01:00<00:41, 10.31s/it]Loading checkpoint shards:  60%|██████    | 6/10 [00:58<00:40, 10.12s/it]Loading checkpoint shards:  60%|██████    | 6/10 [00:49<00:36,  9.19s/it]Loading checkpoint shards:  70%|███████   | 7/10 [01:34<00:35, 11.71s/it]Loading checkpoint shards:  70%|███████   | 7/10 [01:34<00:35, 11.73s/it]Loading checkpoint shards:  70%|███████   | 7/10 [01:35<00:35, 11.76s/it]Loading checkpoint shards:  70%|███████   | 7/10 [01:32<00:34, 11.66s/it]Loading checkpoint shards:  70%|███████   | 7/10 [01:10<00:31, 10.51s/it]Loading checkpoint shards:  70%|███████   | 7/10 [01:12<00:31, 10.64s/it]Loading checkpoint shards:  70%|███████   | 7/10 [01:12<00:31, 10.64s/it]Loading checkpoint shards:  70%|███████   | 7/10 [01:00<00:29,  9.90s/it]Loading checkpoint shards:  80%|████████  | 8/10 [01:43<00:22, 11.27s/it]Loading checkpoint shards:  80%|████████  | 8/10 [01:45<00:22, 11.34s/it]Loading checkpoint shards:  80%|████████  | 8/10 [01:45<00:22, 11.33s/it]Loading checkpoint shards:  80%|████████  | 8/10 [01:44<00:22, 11.32s/it]Loading checkpoint shards:  80%|████████  | 8/10 [01:20<00:21, 10.55s/it]Loading checkpoint shards:  80%|████████  | 8/10 [01:22<00:21, 10.64s/it]Loading checkpoint shards:  80%|████████  | 8/10 [01:22<00:21, 10.64s/it]Loading checkpoint shards:  80%|████████  | 8/10 [01:11<00:20, 10.17s/it]Loading checkpoint shards:  90%|█████████ | 9/10 [01:50<00:10, 10.15s/it]Loading checkpoint shards:  90%|█████████ | 9/10 [01:53<00:10, 10.20s/it]Loading checkpoint shards:  90%|█████████ | 9/10 [01:52<00:10, 10.19s/it]Loading checkpoint shards:  90%|█████████ | 9/10 [01:52<00:10, 10.19s/it]Loading checkpoint shards:  90%|█████████ | 9/10 [01:28<00:09,  9.65s/it]Loading checkpoint shards:  90%|█████████ | 9/10 [01:30<00:09,  9.75s/it]Loading checkpoint shards:  90%|█████████ | 9/10 [01:30<00:09,  9.75s/it]Loading checkpoint shards:  90%|█████████ | 9/10 [01:18<00:09,  9.43s/it]Loading checkpoint shards: 100%|██████████| 10/10 [04:23<00:00, 54.13s/it]Loading checkpoint shards: 100%|██████████| 10/10 [04:25<00:00, 54.16s/it]Loading checkpoint shards: 100%|██████████| 10/10 [04:25<00:00, 54.16s/it]Loading checkpoint shards: 100%|██████████| 10/10 [04:23<00:00, 26.35s/it]Loading checkpoint shards: 100%|██████████| 10/10 [04:24<00:00, 54.15s/it]
Loading checkpoint shards: 100%|██████████| 10/10 [04:26<00:00, 26.60s/it]
Loading checkpoint shards: 100%|██████████| 10/10 [04:25<00:00, 26.54s/it]
Loading checkpoint shards: 100%|██████████| 10/10 [04:24<00:00, 26.49s/it]
Loading checkpoint shards: 100%|██████████| 10/10 [04:00<00:00, 53.72s/it]Loading checkpoint shards: 100%|██████████| 10/10 [04:00<00:00, 24.10s/it]
Loading checkpoint shards: 100%|██████████| 10/10 [03:51<00:00, 53.47s/it]Loading checkpoint shards: 100%|██████████| 10/10 [04:02<00:00, 53.80s/it]Loading checkpoint shards: 100%|██████████| 10/10 [04:03<00:00, 53.81s/it]Loading checkpoint shards: 100%|██████████| 10/10 [04:03<00:00, 24.31s/it]Loading checkpoint shards: 100%|██████████| 10/10 [03:51<00:00, 23.12s/it]
Loading checkpoint shards: 100%|██████████| 10/10 [04:03<00:00, 24.31s/it]

Loading data...
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None)
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None)
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None)
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None)
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None)
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None)
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None)
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None)
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using /ocean/projects/cis230002p/amartin1/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...Using /ocean/projects/cis230002p/amartin1/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /ocean/projects/cis230002p/amartin1/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /ocean/projects/cis230002p/amartin1/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /ocean/projects/cis230002p/amartin1/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /ocean/projects/cis230002p/amartin1/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /ocean/projects/cis230002p/amartin1/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /ocean/projects/cis230002p/amartin1/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...

Detected CUDA files, patching ldflags
Emitting ninja build file /ocean/projects/cis230002p/amartin1/.cache/torch_extensions/py310_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.20495986938476562 seconds
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.21914434432983398 seconds
Time to load fused_adam op: 0.21948647499084473 seconds
Time to load fused_adam op: 0.21952581405639648 seconds
Time to load fused_adam op: 0.21959280967712402 secondsTime to load fused_adam op: 0.21403884887695312 seconds

Time to load fused_adam op: 0.20981884002685547 seconds
Time to load fused_adam op: 0.2198045253753662 seconds
wandb: Currently logged in as: amaadmartin (athena-innovations). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /ocean/projects/cis240092p/amartin1/ReVL/wandb/run-20240918_210856-40fk06e4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-haze-7
wandb: ⭐️ View project at https://wandb.ai/athena-innovations/revl_k_0
wandb: 🚀 View run at https://wandb.ai/athena-innovations/revl_k_0/runs/40fk06e4
  0%|          | 0/370 [00:00<?, ?it/s]  0%|          | 1/370 [00:43<4:29:08, 43.76s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                 {'loss': 2.0996, 'learning_rate': 0, 'epoch': 0.01}
  0%|          | 1/370 [00:44<4:29:08, 43.76s/it]  1%|          | 2/370 [01:30<4:39:25, 45.56s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                 {'loss': 2.2431, 'learning_rate': 0, 'epoch': 0.03}
  1%|          | 2/370 [01:30<4:39:25, 45.56s/it]  1%|          | 3/370 [02:23<4:58:16, 48.76s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                 {'loss': 2.2433, 'learning_rate': 0, 'epoch': 0.04}
  1%|          | 3/370 [02:23<4:58:16, 48.76s/it]  1%|          | 4/370 [03:16<5:09:16, 50.70s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                 {'loss': 2.163, 'learning_rate': 0, 'epoch': 0.05}
  1%|          | 4/370 [03:16<5:09:16, 50.70s/it]  1%|▏         | 5/370 [04:11<5:16:05, 51.96s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                 {'loss': 2.2208, 'learning_rate': 0, 'epoch': 0.07}
  1%|▏         | 5/370 [04:11<5:16:05, 51.96s/it]  2%|▏         | 6/370 [05:05<5:19:31, 52.67s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                 {'loss': 2.1398, 'learning_rate': 0, 'epoch': 0.08}
  2%|▏         | 6/370 [05:05<5:19:31, 52.67s/it]  2%|▏         | 7/370 [05:48<4:59:27, 49.50s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                 {'loss': 2.1575, 'learning_rate': 0, 'epoch': 0.09}
  2%|▏         | 7/370 [05:48<4:59:27, 49.50s/it]  2%|▏         | 8/370 [06:36<4:55:46, 49.02s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                 {'loss': 2.1784, 'learning_rate': 0, 'epoch': 0.11}
  2%|▏         | 8/370 [06:36<4:55:46, 49.02s/it]  2%|▏         | 9/370 [07:56<5:53:19, 58.72s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                 {'loss': 2.2341, 'learning_rate': 0, 'epoch': 0.12}
  2%|▏         | 9/370 [07:56<5:53:19, 58.72s/it]  3%|▎         | 10/370 [08:44<5:33:09, 55.53s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                  {'loss': 2.2416, 'learning_rate': 0, 'epoch': 0.13}
  3%|▎         | 10/370 [08:44<5:33:09, 55.53s/it]  3%|▎         | 11/370 [09:33<5:20:23, 53.55s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                  {'loss': 2.1789, 'learning_rate': 0, 'epoch': 0.15}
  3%|▎         | 11/370 [09:33<5:20:23, 53.55s/it]  3%|▎         | 12/370 [10:30<5:26:06, 54.66s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                  {'loss': 2.2327, 'learning_rate': 0, 'epoch': 0.16}
  3%|▎         | 12/370 [10:30<5:26:06, 54.66s/it]  4%|▎         | 13/370 [12:07<6:41:12, 67.43s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                  {'loss': 2.2335, 'learning_rate': 0, 'epoch': 0.18}
  4%|▎         | 13/370 [12:07<6:41:12, 67.43s/it]  4%|▍         | 14/370 [13:08<6:29:17, 65.61s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
tried to get lr value before scheduler/optimizer started stepping, returning lr=0
                                                  {'loss': 2.2386, 'learning_rate': 0, 'epoch': 0.19}
  4%|▍         | 14/370 [13:08<6:29:17, 65.61s/it]  4%|▍         | 15/370 [14:00<6:02:26, 61.26s/it]                                                  {'loss': 2.2204, 'learning_rate': 0.0, 'epoch': 0.2}
  4%|▍         | 15/370 [14:00<6:02:26, 61.26s/it]  4%|▍         | 16/370 [15:12<6:21:13, 64.62s/it]                                                  {'loss': 2.1763, 'learning_rate': 5e-06, 'epoch': 0.22}
  4%|▍         | 16/370 [15:12<6:21:13, 64.62s/it]  5%|▍         | 17/370 [15:56<5:42:52, 58.28s/it]                                                  {'loss': 2.1375, 'learning_rate': 7.924812503605782e-06, 'epoch': 0.23}
  5%|▍         | 17/370 [15:56<5:42:52, 58.28s/it]  5%|▍         | 18/370 [16:46<5:28:14, 55.95s/it]                                                  {'loss': 2.2016, 'learning_rate': 1e-05, 'epoch': 0.24}
  5%|▍         | 18/370 [16:46<5:28:14, 55.95s/it]  5%|▌         | 19/370 [17:27<5:01:43, 51.58s/it]                                                  {'loss': 2.1725, 'learning_rate': 1e-05, 'epoch': 0.26}
  5%|▌         | 19/370 [17:28<5:01:43, 51.58s/it]  5%|▌         | 20/370 [18:09<4:43:37, 48.62s/it]                                                  {'loss': 2.1037, 'learning_rate': 1e-05, 'epoch': 0.27}
  5%|▌         | 20/370 [18:09<4:43:37, 48.62s/it]  6%|▌         | 21/370 [18:51<4:30:35, 46.52s/it]                                                  {'loss': 2.0885, 'learning_rate': 1e-05, 'epoch': 0.28}
  6%|▌         | 21/370 [18:51<4:30:35, 46.52s/it]  6%|▌         | 22/370 [19:32<4:21:11, 45.03s/it]                                                  {'loss': 2.0409, 'learning_rate': 1e-05, 'epoch': 0.3}
  6%|▌         | 22/370 [19:32<4:21:11, 45.03s/it]  6%|▌         | 23/370 [20:14<4:14:48, 44.06s/it]                                                  {'loss': 1.9346, 'learning_rate': 1e-05, 'epoch': 0.31}
  6%|▌         | 23/370 [20:14<4:14:48, 44.06s/it]  6%|▋         | 24/370 [21:19<4:49:09, 50.14s/it]                                                  {'loss': 1.8923, 'learning_rate': 1e-05, 'epoch': 0.32}
  6%|▋         | 24/370 [21:19<4:49:09, 50.14s/it]  7%|▋         | 25/370 [22:29<5:23:07, 56.19s/it]                                                  {'loss': 1.7966, 'learning_rate': 1e-05, 'epoch': 0.34}
  7%|▋         | 25/370 [22:29<5:23:07, 56.19s/it]  7%|▋         | 26/370 [23:46<5:58:45, 62.57s/it]                                                  {'loss': 1.7483, 'learning_rate': 1e-05, 'epoch': 0.35}
  7%|▋         | 26/370 [23:46<5:58:45, 62.57s/it]  7%|▋         | 27/370 [24:31<5:27:47, 57.34s/it]                                                  {'loss': 1.7101, 'learning_rate': 1e-05, 'epoch': 0.36}
  7%|▋         | 27/370 [24:31<5:27:47, 57.34s/it]  8%|▊         | 28/370 [25:38<5:43:06, 60.19s/it]                                                  {'loss': 1.6562, 'learning_rate': 1e-05, 'epoch': 0.38}
  8%|▊         | 28/370 [25:38<5:43:06, 60.19s/it]  8%|▊         | 29/370 [26:29<5:26:45, 57.49s/it]                                                  {'loss': 1.5748, 'learning_rate': 1e-05, 'epoch': 0.39}
  8%|▊         | 29/370 [26:30<5:26:45, 57.49s/it]  8%|▊         | 30/370 [27:21<5:16:06, 55.78s/it]                                                  {'loss': 1.5733, 'learning_rate': 1e-05, 'epoch': 0.4}
  8%|▊         | 30/370 [27:21<5:16:06, 55.78s/it]  8%|▊         | 31/370 [28:57<6:22:15, 67.66s/it]                                                  {'loss': 1.5253, 'learning_rate': 1e-05, 'epoch': 0.42}
  8%|▊         | 31/370 [28:57<6:22:15, 67.66s/it]  9%|▊         | 32/370 [30:03<6:18:51, 67.25s/it]                                                  {'loss': 1.4598, 'learning_rate': 1e-05, 'epoch': 0.43}
  9%|▊         | 32/370 [30:03<6:18:51, 67.25s/it]  9%|▉         | 33/370 [32:06<7:51:22, 83.92s/it]                                                  {'loss': 1.4038, 'learning_rate': 1e-05, 'epoch': 0.44}
  9%|▉         | 33/370 [32:06<7:51:22, 83.92s/it]  9%|▉         | 34/370 [33:38<8:03:07, 86.27s/it]                                                  {'loss': 1.3687, 'learning_rate': 1e-05, 'epoch': 0.46}
  9%|▉         | 34/370 [33:38<8:03:07, 86.27s/it]  9%|▉         | 35/370 [34:34<7:11:36, 77.30s/it]                                                  {'loss': 1.3298, 'learning_rate': 1e-05, 'epoch': 0.47}
  9%|▉         | 35/370 [34:34<7:11:36, 77.30s/it] 10%|▉         | 36/370 [35:50<7:07:36, 76.82s/it]                                                  {'loss': 1.2961, 'learning_rate': 1e-05, 'epoch': 0.48}
 10%|▉         | 36/370 [35:50<7:07:36, 76.82s/it] 10%|█         | 37/370 [37:27<7:41:07, 83.08s/it]                                                  {'loss': 1.2649, 'learning_rate': 1e-05, 'epoch': 0.5}
 10%|█         | 37/370 [37:27<7:41:07, 83.08s/it] 10%|█         | 38/370 [38:40<7:22:11, 79.91s/it]                                                  {'loss': 1.2455, 'learning_rate': 1e-05, 'epoch': 0.51}
 10%|█         | 38/370 [38:40<7:22:11, 79.91s/it] 11%|█         | 39/370 [39:39<6:45:59, 73.59s/it]                                                  {'loss': 1.1933, 'learning_rate': 1e-05, 'epoch': 0.53}
 11%|█         | 39/370 [39:39<6:45:59, 73.59s/it] 11%|█         | 40/370 [40:53<6:46:08, 73.84s/it]                                                  {'loss': 1.1846, 'learning_rate': 1e-05, 'epoch': 0.54}
 11%|█         | 40/370 [40:53<6:46:08, 73.84s/it] 11%|█         | 41/370 [42:52<7:59:12, 87.39s/it]                                                  {'loss': 1.1375, 'learning_rate': 1e-05, 'epoch': 0.55}
 11%|█         | 41/370 [42:52<7:59:12, 87.39s/it] 11%|█▏        | 42/370 [43:39<6:50:39, 75.12s/it]                                                  {'loss': 1.1227, 'learning_rate': 1e-05, 'epoch': 0.57}
 11%|█▏        | 42/370 [43:39<6:50:39, 75.12s/it] 12%|█▏        | 43/370 [44:46<6:37:26, 72.92s/it]                                                  {'loss': 1.1124, 'learning_rate': 1e-05, 'epoch': 0.58}
 12%|█▏        | 43/370 [44:46<6:37:26, 72.92s/it] 12%|█▏        | 44/370 [46:09<6:52:51, 75.99s/it]                                                  {'loss': 1.1066, 'learning_rate': 1e-05, 'epoch': 0.59}
 12%|█▏        | 44/370 [46:10<6:52:51, 75.99s/it] 12%|█▏        | 45/370 [48:03<7:52:30, 87.23s/it]                                                  {'loss': 1.0742, 'learning_rate': 1e-05, 'epoch': 0.61}
 12%|█▏        | 45/370 [48:03<7:52:30, 87.23s/it] 12%|█▏        | 46/370 [48:44<6:36:41, 73.46s/it]                                                  {'loss': 1.0335, 'learning_rate': 1e-05, 'epoch': 0.62}
 12%|█▏        | 46/370 [48:44<6:36:41, 73.46s/it] 13%|█▎        | 47/370 [49:27<5:45:28, 64.18s/it]                                                  {'loss': 1.0114, 'learning_rate': 1e-05, 'epoch': 0.63}
 13%|█▎        | 47/370 [49:27<5:45:28, 64.18s/it] 13%|█▎        | 48/370 [50:18<5:23:50, 60.34s/it]                                                  {'loss': 0.9908, 'learning_rate': 1e-05, 'epoch': 0.65}
 13%|█▎        | 48/370 [50:18<5:23:50, 60.34s/it] 13%|█▎        | 49/370 [51:06<5:02:26, 56.53s/it]                                                  {'loss': 0.9646, 'learning_rate': 1e-05, 'epoch': 0.66}
 13%|█▎        | 49/370 [51:06<5:02:26, 56.53s/it] 14%|█▎        | 50/370 [52:01<5:00:03, 56.26s/it]                                                  {'loss': 0.9566, 'learning_rate': 1e-05, 'epoch': 0.67}
 14%|█▎        | 50/370 [52:01<5:00:03, 56.26s/it] 14%|█▍        | 51/370 [52:43<4:35:50, 51.88s/it]                                                  {'loss': 0.9247, 'learning_rate': 1e-05, 'epoch': 0.69}
 14%|█▍        | 51/370 [52:43<4:35:50, 51.88s/it] 14%|█▍        | 52/370 [53:25<4:19:20, 48.93s/it]                                                  {'loss': 0.9004, 'learning_rate': 1e-05, 'epoch': 0.7}
 14%|█▍        | 52/370 [53:25<4:19:20, 48.93s/it] 14%|█▍        | 53/370 [54:07<4:07:48, 46.91s/it]                                                  {'loss': 0.8739, 'learning_rate': 1e-05, 'epoch': 0.71}
 14%|█▍        | 53/370 [54:07<4:07:48, 46.91s/it] 15%|█▍        | 54/370 [54:50<3:59:49, 45.54s/it]                                                  {'loss': 0.8482, 'learning_rate': 1e-05, 'epoch': 0.73}
 15%|█▍        | 54/370 [54:50<3:59:49, 45.54s/it] 15%|█▍        | 55/370 [55:51<4:24:11, 50.32s/it]                                                  {'loss': 0.8459, 'learning_rate': 1e-05, 'epoch': 0.74}
 15%|█▍        | 55/370 [55:51<4:24:11, 50.32s/it] 15%|█▌        | 56/370 [56:34<4:11:33, 48.07s/it]                                                  {'loss': 0.8382, 'learning_rate': 1e-05, 'epoch': 0.75}
 15%|█▌        | 56/370 [56:34<4:11:33, 48.07s/it] 15%|█▌        | 57/370 [57:48<4:51:00, 55.78s/it]                                                  {'loss': 0.7775, 'learning_rate': 1e-05, 'epoch': 0.77}
 15%|█▌        | 57/370 [57:48<4:51:00, 55.78s/it] 16%|█▌        | 58/370 [59:52<6:37:22, 76.42s/it]                                                  {'loss': 0.7837, 'learning_rate': 1e-05, 'epoch': 0.78}
 16%|█▌        | 58/370 [59:52<6:37:22, 76.42s/it] 16%|█▌        | 59/370 [1:01:06<6:31:53, 75.61s/it]                                                    {'loss': 0.7619, 'learning_rate': 1e-05, 'epoch': 0.79}
 16%|█▌        | 59/370 [1:01:06<6:31:53, 75.61s/it] 16%|█▌        | 60/370 [1:01:50<5:41:15, 66.05s/it]                                                    {'loss': 0.7275, 'learning_rate': 1e-05, 'epoch': 0.81}
 16%|█▌        | 60/370 [1:01:50<5:41:15, 66.05s/it]
  0%|          | 0/63 [00:00<?, ?it/s][A
  3%|▎         | 2/63 [00:01<00:50,  1.20it/s][A
  5%|▍         | 3/63 [00:03<01:03,  1.06s/it][A
  6%|▋         | 4/63 [00:04<01:10,  1.19s/it][A
  8%|▊         | 5/63 [00:05<01:14,  1.29s/it][A
 10%|▉         | 6/63 [00:07<01:21,  1.43s/it][A
 11%|█         | 7/63 [00:09<01:20,  1.43s/it][A
 13%|█▎        | 8/63 [00:10<01:20,  1.46s/it][A
 14%|█▍        | 9/63 [00:11<01:17,  1.44s/it][A
 16%|█▌        | 10/63 [00:13<01:17,  1.46s/it][A
 17%|█▋        | 11/63 [00:14<01:14,  1.44s/it][A
 19%|█▉        | 12/63 [00:16<01:14,  1.46s/it][A
 21%|██        | 13/63 [00:17<01:13,  1.47s/it][A
 22%|██▏       | 14/63 [00:38<05:53,  7.21s/it][A
 24%|██▍       | 15/63 [00:39<04:23,  5.50s/it][A
 25%|██▌       | 16/63 [00:41<03:20,  4.27s/it][A
 27%|██▋       | 17/63 [00:42<02:38,  3.44s/it][A
 29%|██▊       | 18/63 [00:44<02:07,  2.84s/it][A
 30%|███       | 19/63 [00:45<01:46,  2.43s/it][A
 32%|███▏      | 20/63 [00:48<01:47,  2.50s/it][A
 33%|███▎      | 21/63 [00:49<01:29,  2.14s/it][A
 35%|███▍      | 22/63 [00:51<01:18,  1.91s/it][A
 37%|███▋      | 23/63 [00:52<01:11,  1.78s/it][A
 38%|███▊      | 24/63 [00:54<01:11,  1.83s/it][A
 40%|███▉      | 25/63 [00:55<01:04,  1.69s/it][A
 41%|████▏     | 26/63 [00:57<01:00,  1.64s/it][A
 43%|████▎     | 27/63 [00:58<00:57,  1.60s/it][A
 44%|████▍     | 28/63 [01:00<00:55,  1.59s/it][A
 46%|████▌     | 29/63 [01:36<06:39, 11.76s/it][A
 48%|████▊     | 30/63 [01:37<04:49,  8.78s/it][A
 49%|████▉     | 31/63 [01:39<03:31,  6.59s/it][A
 51%|█████     | 32/63 [01:40<02:35,  5.03s/it][A
 52%|█████▏    | 33/63 [01:42<01:58,  3.95s/it][A
 54%|█████▍    | 34/63 [01:43<01:32,  3.20s/it][A
 56%|█████▌    | 35/63 [01:45<01:15,  2.71s/it][A
 57%|█████▋    | 36/63 [01:46<01:01,  2.29s/it][A
 59%|█████▊    | 37/63 [01:47<00:53,  2.04s/it][A
 60%|██████    | 38/63 [01:49<00:46,  1.85s/it][A
 62%|██████▏   | 39/63 [01:50<00:42,  1.78s/it][A
 63%|██████▎   | 40/63 [01:52<00:38,  1.68s/it][A
 65%|██████▌   | 41/63 [01:53<00:35,  1.60s/it][A
 67%|██████▋   | 42/63 [01:55<00:33,  1.60s/it][A
 68%|██████▊   | 43/63 [01:56<00:30,  1.51s/it][A
 70%|██████▉   | 44/63 [01:58<00:28,  1.52s/it][A
 71%|███████▏  | 45/63 [01:59<00:26,  1.46s/it][A
 73%|███████▎  | 46/63 [02:01<00:25,  1.50s/it][A
 75%|███████▍  | 47/63 [02:02<00:23,  1.49s/it][A
 76%|███████▌  | 48/63 [02:04<00:22,  1.50s/it][A
 78%|███████▊  | 49/63 [02:05<00:20,  1.45s/it][A
 79%|███████▉  | 50/63 [02:06<00:19,  1.47s/it][A
 81%|████████  | 51/63 [02:08<00:17,  1.45s/it][A
 83%|████████▎ | 52/63 [02:09<00:16,  1.47s/it][A
 84%|████████▍ | 53/63 [02:11<00:14,  1.49s/it][A
 86%|████████▌ | 54/63 [02:12<00:13,  1.47s/it][A
 87%|████████▋ | 55/63 [02:14<00:11,  1.48s/it][A
 89%|████████▉ | 56/63 [02:15<00:09,  1.42s/it][A
 90%|█████████ | 57/63 [02:17<00:08,  1.48s/it][A
 92%|█████████▏| 58/63 [02:18<00:07,  1.44s/it][A
 94%|█████████▎| 59/63 [02:20<00:05,  1.46s/it][A
 95%|█████████▌| 60/63 [02:21<00:04,  1.44s/it][A
 97%|█████████▋| 61/63 [02:23<00:02,  1.47s/it][A
 98%|█████████▊| 62/63 [02:24<00:01,  1.44s/it][A
100%|██████████| 63/63 [02:29<00:00,  2.39s/it][A                                                    
                                               [A{'eval_loss': 0.71240234375, 'eval_runtime': 150.9083, 'eval_samples_per_second': 3.313, 'eval_steps_per_second': 0.417, 'epoch': 0.81}
 16%|█▌        | 60/370 [1:04:21<5:41:15, 66.05s/it]
100%|██████████| 63/63 [02:29<00:00,  2.39s/it][A
                                               [A/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 16%|█▋        | 61/370 [1:25:40<40:47:29, 475.24s/it]                                                      {'loss': 0.7352, 'learning_rate': 1e-05, 'epoch': 0.82}
 16%|█▋        | 61/370 [1:25:40<40:47:29, 475.24s/it] 17%|█▋        | 62/370 [1:27:22<31:05:42, 363.45s/it]                                                      {'loss': 0.6883, 'learning_rate': 1e-05, 'epoch': 0.84}
 17%|█▋        | 62/370 [1:27:22<31:05:42, 363.45s/it] 17%|█▋        | 63/370 [1:28:43<23:45:51, 278.67s/it]                                                      {'loss': 0.6633, 'learning_rate': 1e-05, 'epoch': 0.85}
 17%|█▋        | 63/370 [1:28:43<23:45:51, 278.67s/it] 17%|█▋        | 64/370 [1:31:05<20:12:16, 237.70s/it]                                                      {'loss': 0.6521, 'learning_rate': 1e-05, 'epoch': 0.86}
 17%|█▋        | 64/370 [1:31:05<20:12:16, 237.70s/it] 18%|█▊        | 65/370 [1:32:06<15:37:43, 184.47s/it]                                                      {'loss': 0.6287, 'learning_rate': 1e-05, 'epoch': 0.88}
 18%|█▊        | 65/370 [1:32:06<15:37:43, 184.47s/it] 18%|█▊        | 66/370 [1:33:21<12:48:22, 151.65s/it]                                                      {'loss': 0.6261, 'learning_rate': 1e-05, 'epoch': 0.89}
 18%|█▊        | 66/370 [1:33:21<12:48:22, 151.65s/it] 18%|█▊        | 67/370 [1:34:05<10:03:36, 119.53s/it]                                                      {'loss': 0.5967, 'learning_rate': 1e-05, 'epoch': 0.9}
 18%|█▊        | 67/370 [1:34:05<10:03:36, 119.53s/it] 18%|█▊        | 68/370 [1:34:48<8:05:33, 96.47s/it]                                                      {'loss': 0.5798, 'learning_rate': 1e-05, 'epoch': 0.92}
 18%|█▊        | 68/370 [1:34:48<8:05:33, 96.47s/it] 19%|█▊        | 69/370 [1:36:02<7:29:36, 89.62s/it]                                                    {'loss': 0.5654, 'learning_rate': 1e-05, 'epoch': 0.93}
 19%|█▊        | 69/370 [1:36:02<7:29:36, 89.62s/it] 19%|█▉        | 70/370 [1:37:02<6:44:54, 80.98s/it]                                                    {'loss': 0.5508, 'learning_rate': 1e-05, 'epoch': 0.94}
 19%|█▉        | 70/370 [1:37:02<6:44:54, 80.98s/it] 19%|█▉        | 71/370 [1:38:07<6:18:26, 75.94s/it]                                                    {'loss': 0.5446, 'learning_rate': 1e-05, 'epoch': 0.96}
 19%|█▉        | 71/370 [1:38:07<6:18:26, 75.94s/it] 19%|█▉        | 72/370 [1:38:49<5:27:09, 65.87s/it]                                                    {'loss': 0.55, 'learning_rate': 1e-05, 'epoch': 0.97}
 19%|█▉        | 72/370 [1:38:49<5:27:09, 65.87s/it] 20%|█▉        | 73/370 [1:40:57<6:58:16, 84.50s/it]                                                    {'loss': 0.5475, 'learning_rate': 1e-05, 'epoch': 0.98}
 20%|█▉        | 73/370 [1:40:57<6:58:16, 84.50s/it] 20%|██        | 74/370 [1:41:53<6:14:32, 75.92s/it]                                                    {'loss': 0.5462, 'learning_rate': 1e-05, 'epoch': 1.0}
 20%|██        | 74/370 [1:41:53<6:14:32, 75.92s/it] 20%|██        | 75/370 [1:43:12<6:18:08, 76.91s/it]                                                    {'loss': 0.5226, 'learning_rate': 1e-05, 'epoch': 1.01}
 20%|██        | 75/370 [1:43:12<6:18:08, 76.91s/it] 21%|██        | 76/370 [1:43:53<5:24:26, 66.21s/it]                                                    {'loss': 0.4981, 'learning_rate': 1e-05, 'epoch': 1.02}
 21%|██        | 76/370 [1:43:53<5:24:26, 66.21s/it] 21%|██        | 77/370 [1:44:35<4:46:43, 58.71s/it]                                                    {'loss': 0.5191, 'learning_rate': 1e-05, 'epoch': 1.04}
 21%|██        | 77/370 [1:44:35<4:46:43, 58.71s/it] 21%|██        | 78/370 [1:45:16<4:20:44, 53.58s/it]                                                    {'loss': 0.5124, 'learning_rate': 1e-05, 'epoch': 1.05}
 21%|██        | 78/370 [1:45:16<4:20:44, 53.58s/it] 21%|██▏       | 79/370 [1:45:57<4:01:41, 49.83s/it]                                                    {'loss': 0.5145, 'learning_rate': 1e-05, 'epoch': 1.06}
 21%|██▏       | 79/370 [1:45:57<4:01:41, 49.83s/it] 22%|██▏       | 80/370 [1:46:39<3:48:50, 47.35s/it]                                                    {'loss': 0.5093, 'learning_rate': 1e-05, 'epoch': 1.08}
 22%|██▏       | 80/370 [1:46:39<3:48:50, 47.35s/it] 22%|██▏       | 81/370 [1:47:20<3:39:07, 45.49s/it]                                                    {'loss': 0.4952, 'learning_rate': 1e-05, 'epoch': 1.09}
 22%|██▏       | 81/370 [1:47:20<3:39:07, 45.49s/it] 22%|██▏       | 82/370 [1:48:02<3:32:46, 44.33s/it]                                                    {'loss': 0.482, 'learning_rate': 1e-05, 'epoch': 1.1}
 22%|██▏       | 82/370 [1:48:02<3:32:46, 44.33s/it] 22%|██▏       | 83/370 [1:48:44<3:29:07, 43.72s/it]                                                    {'loss': 0.4983, 'learning_rate': 1e-05, 'epoch': 1.12}
 22%|██▏       | 83/370 [1:48:44<3:29:07, 43.72s/it] 23%|██▎       | 84/370 [1:49:26<3:26:20, 43.29s/it]                                                    {'loss': 0.5018, 'learning_rate': 1e-05, 'epoch': 1.13}
 23%|██▎       | 84/370 [1:49:26<3:26:20, 43.29s/it] 23%|██▎       | 85/370 [1:50:07<3:22:00, 42.53s/it]                                                    {'loss': 0.4819, 'learning_rate': 1e-05, 'epoch': 1.14}
 23%|██▎       | 85/370 [1:50:07<3:22:00, 42.53s/it] 23%|██▎       | 86/370 [1:50:49<3:20:21, 42.33s/it]                                                    {'loss': 0.4939, 'learning_rate': 1e-05, 'epoch': 1.16}
 23%|██▎       | 86/370 [1:50:49<3:20:21, 42.33s/it] 24%|██▎       | 87/370 [1:51:30<3:18:44, 42.14s/it]                                                    {'loss': 0.479, 'learning_rate': 1e-05, 'epoch': 1.17}
 24%|██▎       | 87/370 [1:51:30<3:18:44, 42.14s/it] 24%|██▍       | 88/370 [1:52:11<3:15:43, 41.65s/it]                                                    {'loss': 0.4974, 'learning_rate': 1e-05, 'epoch': 1.19}
 24%|██▍       | 88/370 [1:52:11<3:15:43, 41.65s/it] 24%|██▍       | 89/370 [1:52:53<3:15:25, 41.73s/it]                                                    {'loss': 0.4841, 'learning_rate': 1e-05, 'epoch': 1.2}
 24%|██▍       | 89/370 [1:52:53<3:15:25, 41.73s/it] 24%|██▍       | 90/370 [1:53:33<3:13:09, 41.39s/it]                                                    {'loss': 0.4827, 'learning_rate': 1e-05, 'epoch': 1.21}
 24%|██▍       | 90/370 [1:53:34<3:13:09, 41.39s/it] 25%|██▍       | 91/370 [1:54:15<3:12:44, 41.45s/it]                                                    {'loss': 0.4845, 'learning_rate': 1e-05, 'epoch': 1.23}
 25%|██▍       | 91/370 [1:54:15<3:12:44, 41.45s/it] 25%|██▍       | 92/370 [1:54:56<3:11:32, 41.34s/it]                                                    {'loss': 0.4552, 'learning_rate': 1e-05, 'epoch': 1.24}
 25%|██▍       | 92/370 [1:54:56<3:11:32, 41.34s/it] 25%|██▌       | 93/370 [1:55:41<3:16:05, 42.48s/it]                                                    {'loss': 0.4762, 'learning_rate': 1e-05, 'epoch': 1.25}
 25%|██▌       | 93/370 [1:55:41<3:16:05, 42.48s/it] 25%|██▌       | 94/370 [1:56:23<3:13:50, 42.14s/it]                                                    {'loss': 0.4774, 'learning_rate': 1e-05, 'epoch': 1.27}
 25%|██▌       | 94/370 [1:56:23<3:13:50, 42.14s/it] 26%|██▌       | 95/370 [1:57:04<3:12:14, 41.94s/it]                                                    {'loss': 0.4724, 'learning_rate': 1e-05, 'epoch': 1.28}
 26%|██▌       | 95/370 [1:57:04<3:12:14, 41.94s/it] 26%|██▌       | 96/370 [1:57:46<3:11:37, 41.96s/it]                                                    {'loss': 0.4852, 'learning_rate': 1e-05, 'epoch': 1.29}
 26%|██▌       | 96/370 [1:57:46<3:11:37, 41.96s/it] 26%|██▌       | 97/370 [1:58:29<3:11:41, 42.13s/it]                                                    {'loss': 0.4676, 'learning_rate': 1e-05, 'epoch': 1.31}
 26%|██▌       | 97/370 [1:58:29<3:11:41, 42.13s/it] 26%|██▋       | 98/370 [1:59:13<3:14:05, 42.82s/it]                                                    {'loss': 0.458, 'learning_rate': 1e-05, 'epoch': 1.32}
 26%|██▋       | 98/370 [1:59:13<3:14:05, 42.82s/it] 27%|██▋       | 99/370 [1:59:54<3:10:40, 42.22s/it]                                                    {'loss': 0.4622, 'learning_rate': 1e-05, 'epoch': 1.33}
 27%|██▋       | 99/370 [1:59:54<3:10:40, 42.22s/it] 27%|██▋       | 100/370 [2:00:36<3:09:46, 42.17s/it]                                                     {'loss': 0.4597, 'learning_rate': 1e-05, 'epoch': 1.35}
 27%|██▋       | 100/370 [2:00:36<3:09:46, 42.17s/it] 27%|██▋       | 101/370 [2:01:19<3:10:16, 42.44s/it]                                                     {'loss': 0.4636, 'learning_rate': 1e-05, 'epoch': 1.36}
 27%|██▋       | 101/370 [2:01:19<3:10:16, 42.44s/it] 28%|██▊       | 102/370 [2:02:01<3:08:35, 42.22s/it]                                                     {'loss': 0.4593, 'learning_rate': 1e-05, 'epoch': 1.37}
 28%|██▊       | 102/370 [2:02:01<3:08:35, 42.22s/it] 28%|██▊       | 103/370 [2:02:44<3:09:01, 42.48s/it]                                                     {'loss': 0.4529, 'learning_rate': 1e-05, 'epoch': 1.39}
 28%|██▊       | 103/370 [2:02:44<3:09:01, 42.48s/it] 28%|██▊       | 104/370 [2:03:25<3:06:15, 42.01s/it]                                                     {'loss': 0.4632, 'learning_rate': 1e-05, 'epoch': 1.4}
 28%|██▊       | 104/370 [2:03:25<3:06:15, 42.01s/it] 28%|██▊       | 105/370 [2:04:08<3:07:14, 42.40s/it]                                                     {'loss': 0.451, 'learning_rate': 1e-05, 'epoch': 1.41}
 28%|██▊       | 105/370 [2:04:08<3:07:14, 42.40s/it] 29%|██▊       | 106/370 [2:04:49<3:04:33, 41.95s/it]                                                     {'loss': 0.4545, 'learning_rate': 1e-05, 'epoch': 1.43}
 29%|██▊       | 106/370 [2:04:49<3:04:33, 41.95s/it] 29%|██▉       | 107/370 [2:05:31<3:04:10, 42.02s/it]                                                     {'loss': 0.4622, 'learning_rate': 1e-05, 'epoch': 1.44}
 29%|██▉       | 107/370 [2:05:31<3:04:10, 42.02s/it] 29%|██▉       | 108/370 [2:06:14<3:04:20, 42.22s/it]                                                     {'loss': 0.469, 'learning_rate': 1e-05, 'epoch': 1.45}
 29%|██▉       | 108/370 [2:06:14<3:04:20, 42.22s/it] 29%|██▉       | 109/370 [2:06:55<3:02:11, 41.88s/it]                                                     {'loss': 0.4567, 'learning_rate': 1e-05, 'epoch': 1.47}
 29%|██▉       | 109/370 [2:06:55<3:02:11, 41.88s/it] 30%|██▉       | 110/370 [2:08:30<4:10:35, 57.83s/it]                                                     {'loss': 0.4588, 'learning_rate': 1e-05, 'epoch': 1.48}
 30%|██▉       | 110/370 [2:08:30<4:10:35, 57.83s/it] 30%|███       | 111/370 [2:09:12<3:49:36, 53.19s/it]                                                     {'loss': 0.4591, 'learning_rate': 1e-05, 'epoch': 1.49}
 30%|███       | 111/370 [2:09:12<3:49:36, 53.19s/it] 30%|███       | 112/370 [2:09:53<3:32:51, 49.50s/it]                                                     {'loss': 0.45, 'learning_rate': 1e-05, 'epoch': 1.51}
 30%|███       | 112/370 [2:09:53<3:32:51, 49.50s/it] 31%|███       | 113/370 [2:10:36<3:23:10, 47.43s/it]                                                     {'loss': 0.4463, 'learning_rate': 1e-05, 'epoch': 1.52}
 31%|███       | 113/370 [2:10:36<3:23:10, 47.43s/it] 31%|███       | 114/370 [2:11:17<3:14:26, 45.57s/it]                                                     {'loss': 0.4634, 'learning_rate': 1e-05, 'epoch': 1.54}
 31%|███       | 114/370 [2:11:17<3:14:26, 45.57s/it] 31%|███       | 115/370 [2:11:58<3:08:06, 44.26s/it]                                                     {'loss': 0.4478, 'learning_rate': 1e-05, 'epoch': 1.55}
 31%|███       | 115/370 [2:11:58<3:08:06, 44.26s/it] 31%|███▏      | 116/370 [2:12:39<3:03:30, 43.35s/it]                                                     {'loss': 0.4499, 'learning_rate': 1e-05, 'epoch': 1.56}
 31%|███▏      | 116/370 [2:12:39<3:03:30, 43.35s/it] 32%|███▏      | 117/370 [2:13:21<3:00:18, 42.76s/it]                                                     {'loss': 0.4518, 'learning_rate': 1e-05, 'epoch': 1.58}
 32%|███▏      | 117/370 [2:13:21<3:00:18, 42.76s/it] 32%|███▏      | 118/370 [2:14:02<2:57:11, 42.19s/it]                                                     {'loss': 0.4525, 'learning_rate': 1e-05, 'epoch': 1.59}
 32%|███▏      | 118/370 [2:14:02<2:57:11, 42.19s/it] 32%|███▏      | 119/370 [2:14:43<2:55:03, 41.85s/it]                                                     {'loss': 0.4456, 'learning_rate': 1e-05, 'epoch': 1.6}
 32%|███▏      | 119/370 [2:14:43<2:55:03, 41.85s/it] 32%|███▏      | 120/370 [2:15:28<2:59:12, 43.01s/it]                                                     {'loss': 0.4559, 'learning_rate': 1e-05, 'epoch': 1.62}
 32%|███▏      | 120/370 [2:15:28<2:59:12, 43.01s/it]
  0%|          | 0/63 [00:00<?, ?it/s][A
  3%|▎         | 2/63 [00:01<00:30,  1.98it/s][A
  5%|▍         | 3/63 [00:02<00:42,  1.40it/s][A
  6%|▋         | 4/63 [00:03<00:48,  1.22it/s][A
  8%|▊         | 5/63 [00:04<00:52,  1.10it/s][A
 10%|▉         | 6/63 [00:05<00:55,  1.04it/s][A
 11%|█         | 7/63 [00:15<03:47,  4.07s/it][A
 13%|█▎        | 8/63 [00:16<02:49,  3.09s/it][A
 14%|█▍        | 9/63 [00:17<02:12,  2.45s/it][A
 16%|█▌        | 10/63 [00:18<01:45,  1.99s/it][A
 17%|█▋        | 11/63 [00:19<01:28,  1.70s/it][A
 19%|█▉        | 12/63 [00:20<01:15,  1.48s/it][A
 21%|██        | 13/63 [00:21<01:06,  1.33s/it][A
 22%|██▏       | 14/63 [00:22<01:00,  1.22s/it][A
 24%|██▍       | 15/63 [00:23<00:55,  1.17s/it][A
 25%|██▌       | 16/63 [00:24<00:53,  1.15s/it][A
 27%|██▋       | 17/63 [00:25<00:51,  1.11s/it][A
 29%|██▊       | 18/63 [00:26<00:48,  1.08s/it][A
 30%|███       | 19/63 [00:27<00:46,  1.05s/it][A
 32%|███▏      | 20/63 [00:28<00:44,  1.04s/it][A
 33%|███▎      | 21/63 [00:29<00:42,  1.02s/it][A
 35%|███▍      | 22/63 [00:30<00:42,  1.03s/it][A
 37%|███▋      | 23/63 [00:32<00:42,  1.06s/it][A
 38%|███▊      | 24/63 [00:33<00:40,  1.05s/it][A
 40%|███▉      | 25/63 [00:34<00:38,  1.02s/it][A
 41%|████▏     | 26/63 [00:35<00:40,  1.08s/it][A
 43%|████▎     | 27/63 [00:36<00:37,  1.04s/it][A
 44%|████▍     | 28/63 [00:37<00:36,  1.03s/it][A
 46%|████▌     | 29/63 [00:38<00:34,  1.01s/it][A
 48%|████▊     | 30/63 [00:39<00:34,  1.05s/it][A
 49%|████▉     | 31/63 [00:40<00:32,  1.02s/it][A
 51%|█████     | 32/63 [00:41<00:31,  1.02s/it][A
 52%|█████▏    | 33/63 [00:42<00:30,  1.01s/it][A
 54%|█████▍    | 34/63 [00:43<00:29,  1.02s/it][A
 56%|█████▌    | 35/63 [00:44<00:27,  1.01it/s][A
 57%|█████▋    | 36/63 [00:45<00:29,  1.08s/it][A
 59%|█████▊    | 37/63 [00:46<00:27,  1.06s/it][A
 60%|██████    | 38/63 [00:47<00:26,  1.07s/it][A
 62%|██████▏   | 39/63 [00:48<00:25,  1.05s/it][A
 63%|██████▎   | 40/63 [00:49<00:24,  1.08s/it][A
 65%|██████▌   | 41/63 [00:50<00:23,  1.05s/it][A
 67%|██████▋   | 42/63 [00:51<00:21,  1.03s/it][A
 68%|██████▊   | 43/63 [00:52<00:21,  1.09s/it][A
 70%|██████▉   | 44/63 [00:53<00:20,  1.07s/it][A
 71%|███████▏  | 45/63 [00:55<00:18,  1.05s/it][A
 73%|███████▎  | 46/63 [00:56<00:18,  1.08s/it][A
 75%|███████▍  | 47/63 [00:57<00:16,  1.05s/it][A
 76%|███████▌  | 48/63 [00:58<00:15,  1.04s/it][A
 78%|███████▊  | 49/63 [00:59<00:14,  1.03s/it][A
 79%|███████▉  | 50/63 [01:00<00:13,  1.04s/it][A
 81%|████████  | 51/63 [01:01<00:13,  1.12s/it][A
 83%|████████▎ | 52/63 [01:02<00:12,  1.10s/it][A
 84%|████████▍ | 53/63 [01:03<00:10,  1.07s/it][A
 86%|████████▌ | 54/63 [01:04<00:09,  1.08s/it][A
 87%|████████▋ | 55/63 [01:05<00:08,  1.05s/it][A
 89%|████████▉ | 56/63 [01:07<00:08,  1.15s/it][A
 90%|█████████ | 57/63 [01:08<00:06,  1.11s/it][A
 92%|█████████▏| 58/63 [01:09<00:05,  1.08s/it][A
 94%|█████████▎| 59/63 [01:10<00:04,  1.06s/it][A
 95%|█████████▌| 60/63 [01:11<00:03,  1.04s/it][A
 97%|█████████▋| 61/63 [01:12<00:01,  1.00it/s][A
 98%|█████████▊| 62/63 [01:13<00:01,  1.13s/it][A
100%|██████████| 63/63 [01:14<00:00,  1.08s/it][A                                                     
                                               [A{'eval_loss': 0.44921875, 'eval_runtime': 75.4477, 'eval_samples_per_second': 6.627, 'eval_steps_per_second': 0.835, 'epoch': 1.62}
 32%|███▏      | 120/370 [2:16:44<2:59:12, 43.01s/it]
100%|██████████| 63/63 [01:14<00:00,  1.08s/it][A
                                               [A/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 33%|███▎      | 121/370 [2:23:17<11:48:02, 170.61s/it]                                                       {'loss': 0.4426, 'learning_rate': 1e-05, 'epoch': 1.63}
 33%|███▎      | 121/370 [2:23:17<11:48:02, 170.61s/it] 33%|███▎      | 122/370 [2:23:58<9:05:02, 131.87s/it]                                                       {'loss': 0.4458, 'learning_rate': 1e-05, 'epoch': 1.64}
 33%|███▎      | 122/370 [2:23:58<9:05:02, 131.87s/it] 33%|███▎      | 123/370 [2:24:40<7:12:02, 104.95s/it]                                                      {'loss': 0.4477, 'learning_rate': 1e-05, 'epoch': 1.66}
 33%|███▎      | 123/370 [2:24:40<7:12:02, 104.95s/it] 34%|███▎      | 124/370 [2:25:21<5:51:28, 85.72s/it]                                                      {'loss': 0.4437, 'learning_rate': 1e-05, 'epoch': 1.67}
 34%|███▎      | 124/370 [2:25:21<5:51:28, 85.72s/it] 34%|███▍      | 125/370 [2:26:02<4:55:07, 72.28s/it]                                                     {'loss': 0.4423, 'learning_rate': 1e-05, 'epoch': 1.68}
 34%|███▍      | 125/370 [2:26:02<4:55:07, 72.28s/it] 34%|███▍      | 126/370 [2:26:43<4:16:03, 62.96s/it]                                                     {'loss': 0.4351, 'learning_rate': 1e-05, 'epoch': 1.7}
 34%|███▍      | 126/370 [2:26:43<4:16:03, 62.96s/it] 34%|███▍      | 127/370 [2:27:25<3:49:09, 56.58s/it]                                                     {'loss': 0.4482, 'learning_rate': 1e-05, 'epoch': 1.71}
 34%|███▍      | 127/370 [2:27:25<3:49:09, 56.58s/it] 35%|███▍      | 128/370 [2:28:08<3:31:27, 52.43s/it]                                                     {'loss': 0.4528, 'learning_rate': 1e-05, 'epoch': 1.72}
 35%|███▍      | 128/370 [2:28:08<3:31:27, 52.43s/it] 35%|███▍      | 129/370 [2:28:51<3:19:34, 49.69s/it]                                                     {'loss': 0.4363, 'learning_rate': 1e-05, 'epoch': 1.74}
 35%|███▍      | 129/370 [2:28:51<3:19:34, 49.69s/it] 35%|███▌      | 130/370 [2:29:32<3:08:02, 47.01s/it]                                                     {'loss': 0.4308, 'learning_rate': 1e-05, 'epoch': 1.75}
 35%|███▌      | 130/370 [2:29:32<3:08:02, 47.01s/it] 35%|███▌      | 131/370 [2:30:13<3:00:15, 45.25s/it]                                                     {'loss': 0.4324, 'learning_rate': 1e-05, 'epoch': 1.76}
 35%|███▌      | 131/370 [2:30:13<3:00:15, 45.25s/it] 36%|███▌      | 132/370 [2:30:58<2:58:51, 45.09s/it]                                                     {'loss': 0.4328, 'learning_rate': 1e-05, 'epoch': 1.78}
 36%|███▌      | 132/370 [2:30:58<2:58:51, 45.09s/it] 36%|███▌      | 133/370 [2:31:40<2:54:48, 44.25s/it]                                                     {'loss': 0.4395, 'learning_rate': 1e-05, 'epoch': 1.79}
 36%|███▌      | 133/370 [2:31:40<2:54:48, 44.25s/it] 36%|███▌      | 134/370 [2:32:22<2:50:46, 43.42s/it]                                                     {'loss': 0.4341, 'learning_rate': 1e-05, 'epoch': 1.8}
 36%|███▌      | 134/370 [2:32:22<2:50:46, 43.42s/it] 36%|███▋      | 135/370 [2:33:03<2:48:13, 42.95s/it]                                                     {'loss': 0.4387, 'learning_rate': 1e-05, 'epoch': 1.82}
 36%|███▋      | 135/370 [2:33:03<2:48:13, 42.95s/it] 37%|███▋      | 136/370 [2:33:45<2:45:33, 42.45s/it]                                                     {'loss': 0.4442, 'learning_rate': 1e-05, 'epoch': 1.83}
 37%|███▋      | 136/370 [2:33:45<2:45:33, 42.45s/it] 37%|███▋      | 137/370 [2:34:26<2:43:39, 42.14s/it]                                                     {'loss': 0.4372, 'learning_rate': 1e-05, 'epoch': 1.85}
 37%|███▋      | 137/370 [2:34:26<2:43:39, 42.14s/it] 37%|███▋      | 138/370 [2:35:09<2:43:17, 42.23s/it]                                                     {'loss': 0.4336, 'learning_rate': 1e-05, 'epoch': 1.86}
 37%|███▋      | 138/370 [2:35:09<2:43:17, 42.23s/it] 38%|███▊      | 139/370 [2:35:50<2:41:55, 42.06s/it]                                                     {'loss': 0.4391, 'learning_rate': 1e-05, 'epoch': 1.87}
 38%|███▊      | 139/370 [2:35:50<2:41:55, 42.06s/it] 38%|███▊      | 140/370 [2:36:33<2:41:45, 42.20s/it]                                                     {'loss': 0.437, 'learning_rate': 1e-05, 'epoch': 1.89}
 38%|███▊      | 140/370 [2:36:33<2:41:45, 42.20s/it] 38%|███▊      | 141/370 [2:37:16<2:42:18, 42.53s/it]                                                     {'loss': 0.4259, 'learning_rate': 1e-05, 'epoch': 1.9}
 38%|███▊      | 141/370 [2:37:16<2:42:18, 42.53s/it] 38%|███▊      | 142/370 [2:37:57<2:39:47, 42.05s/it]                                                     {'loss': 0.4356, 'learning_rate': 1e-05, 'epoch': 1.91}
 38%|███▊      | 142/370 [2:37:57<2:39:47, 42.05s/it] 39%|███▊      | 143/370 [2:38:38<2:38:19, 41.85s/it]                                                     {'loss': 0.4403, 'learning_rate': 1e-05, 'epoch': 1.93}
 39%|███▊      | 143/370 [2:38:38<2:38:19, 41.85s/it] 39%|███▉      | 144/370 [2:39:21<2:38:24, 42.05s/it]                                                     {'loss': 0.4304, 'learning_rate': 1e-05, 'epoch': 1.94}
 39%|███▉      | 144/370 [2:39:21<2:38:24, 42.05s/it] 39%|███▉      | 145/370 [2:40:02<2:36:33, 41.75s/it]                                                     {'loss': 0.4238, 'learning_rate': 1e-05, 'epoch': 1.95}
 39%|███▉      | 145/370 [2:40:02<2:36:33, 41.75s/it] 39%|███▉      | 146/370 [2:40:44<2:36:48, 42.00s/it]                                                     {'loss': 0.4335, 'learning_rate': 1e-05, 'epoch': 1.97}
 39%|███▉      | 146/370 [2:40:45<2:36:48, 42.00s/it] 40%|███▉      | 147/370 [2:41:42<2:53:47, 46.76s/it]                                                     {'loss': 0.4317, 'learning_rate': 1e-05, 'epoch': 1.98}
 40%|███▉      | 147/370 [2:41:42<2:53:47, 46.76s/it] 40%|████      | 148/370 [2:42:23<2:46:34, 45.02s/it]                                                     {'loss': 0.4425, 'learning_rate': 1e-05, 'epoch': 1.99}
 40%|████      | 148/370 [2:42:23<2:46:34, 45.02s/it] 40%|████      | 149/370 [2:43:05<2:42:17, 44.06s/it]                                                     {'loss': 0.4262, 'learning_rate': 1e-05, 'epoch': 2.01}
 40%|████      | 149/370 [2:43:05<2:42:17, 44.06s/it] 41%|████      | 150/370 [2:43:46<2:37:42, 43.01s/it]                                                     {'loss': 0.4312, 'learning_rate': 1e-05, 'epoch': 2.02}
 41%|████      | 150/370 [2:43:46<2:37:42, 43.01s/it] 41%|████      | 151/370 [2:44:26<2:33:48, 42.14s/it]                                                     {'loss': 0.4316, 'learning_rate': 1e-05, 'epoch': 2.03}
 41%|████      | 151/370 [2:44:26<2:33:48, 42.14s/it] 41%|████      | 152/370 [2:45:06<2:31:24, 41.67s/it]                                                     {'loss': 0.4323, 'learning_rate': 1e-05, 'epoch': 2.05}
 41%|████      | 152/370 [2:45:06<2:31:24, 41.67s/it] 41%|████▏     | 153/370 [2:45:47<2:29:27, 41.32s/it]                                                     {'loss': 0.432, 'learning_rate': 1e-05, 'epoch': 2.06}
 41%|████▏     | 153/370 [2:45:47<2:29:27, 41.32s/it] 42%|████▏     | 154/370 [2:46:27<2:27:50, 41.07s/it]                                                     {'loss': 0.4297, 'learning_rate': 1e-05, 'epoch': 2.07}
 42%|████▏     | 154/370 [2:46:29<2:27:50, 41.07s/it] 42%|████▏     | 155/370 [2:47:09<2:27:44, 41.23s/it]                                                     {'loss': 0.4272, 'learning_rate': 1e-05, 'epoch': 2.09}
 42%|████▏     | 155/370 [2:47:09<2:27:44, 41.23s/it] 42%|████▏     | 156/370 [2:47:49<2:26:13, 41.00s/it]                                                     {'loss': 0.4347, 'learning_rate': 1e-05, 'epoch': 2.1}
 42%|████▏     | 156/370 [2:47:49<2:26:13, 41.00s/it] 42%|████▏     | 157/370 [2:48:30<2:25:14, 40.91s/it]                                                     {'loss': 0.4221, 'learning_rate': 1e-05, 'epoch': 2.11}
 42%|████▏     | 157/370 [2:48:30<2:25:14, 40.91s/it] 43%|████▎     | 158/370 [2:49:11<2:24:06, 40.79s/it]                                                     {'loss': 0.4211, 'learning_rate': 1e-05, 'epoch': 2.13}
 43%|████▎     | 158/370 [2:49:11<2:24:06, 40.79s/it] 43%|████▎     | 159/370 [2:49:51<2:23:01, 40.67s/it]                                                     {'loss': 0.4192, 'learning_rate': 1e-05, 'epoch': 2.14}
 43%|████▎     | 159/370 [2:49:51<2:23:01, 40.67s/it] 43%|████▎     | 160/370 [2:50:32<2:22:36, 40.75s/it]                                                     {'loss': 0.4324, 'learning_rate': 1e-05, 'epoch': 2.15}
 43%|████▎     | 160/370 [2:50:32<2:22:36, 40.75s/it] 44%|████▎     | 161/370 [2:51:13<2:21:52, 40.73s/it]                                                     {'loss': 0.4235, 'learning_rate': 1e-05, 'epoch': 2.17}
 44%|████▎     | 161/370 [2:51:13<2:21:52, 40.73s/it] 44%|████▍     | 162/370 [2:51:53<2:21:00, 40.68s/it]                                                     {'loss': 0.417, 'learning_rate': 1e-05, 'epoch': 2.18}
 44%|████▍     | 162/370 [2:51:53<2:21:00, 40.68s/it] 44%|████▍     | 163/370 [2:52:34<2:20:51, 40.83s/it]                                                     {'loss': 0.4197, 'learning_rate': 1e-05, 'epoch': 2.2}
 44%|████▍     | 163/370 [2:52:34<2:20:51, 40.83s/it] 44%|████▍     | 164/370 [2:53:15<2:19:31, 40.64s/it]                                                     {'loss': 0.4174, 'learning_rate': 1e-05, 'epoch': 2.21}
 44%|████▍     | 164/370 [2:53:15<2:19:31, 40.64s/it] 45%|████▍     | 165/370 [2:53:55<2:18:29, 40.54s/it]                                                     {'loss': 0.4141, 'learning_rate': 1e-05, 'epoch': 2.22}
 45%|████▍     | 165/370 [2:53:55<2:18:29, 40.54s/it] 45%|████▍     | 166/370 [2:54:35<2:17:41, 40.50s/it]                                                     {'loss': 0.4197, 'learning_rate': 1e-05, 'epoch': 2.24}
 45%|████▍     | 166/370 [2:54:35<2:17:41, 40.50s/it] 45%|████▌     | 167/370 [2:55:16<2:17:05, 40.52s/it]                                                     {'loss': 0.426, 'learning_rate': 1e-05, 'epoch': 2.25}
 45%|████▌     | 167/370 [2:55:16<2:17:05, 40.52s/it] 45%|████▌     | 168/370 [2:55:58<2:17:48, 40.93s/it]                                                     {'loss': 0.4199, 'learning_rate': 1e-05, 'epoch': 2.26}
 45%|████▌     | 168/370 [2:55:58<2:17:48, 40.93s/it] 46%|████▌     | 169/370 [2:56:39<2:17:00, 40.90s/it]                                                     {'loss': 0.415, 'learning_rate': 1e-05, 'epoch': 2.28}
 46%|████▌     | 169/370 [2:56:39<2:17:00, 40.90s/it] 46%|████▌     | 170/370 [2:57:19<2:15:47, 40.74s/it]                                                     {'loss': 0.415, 'learning_rate': 1e-05, 'epoch': 2.29}
 46%|████▌     | 170/370 [2:57:19<2:15:47, 40.74s/it] 46%|████▌     | 171/370 [2:58:00<2:15:02, 40.72s/it]                                                     {'loss': 0.4263, 'learning_rate': 1e-05, 'epoch': 2.3}
 46%|████▌     | 171/370 [2:58:00<2:15:02, 40.72s/it] 46%|████▋     | 172/370 [2:58:40<2:13:53, 40.57s/it]                                                     {'loss': 0.4194, 'learning_rate': 1e-05, 'epoch': 2.32}
 46%|████▋     | 172/370 [2:58:40<2:13:53, 40.57s/it] 47%|████▋     | 173/370 [2:59:20<2:13:09, 40.56s/it]                                                     {'loss': 0.4098, 'learning_rate': 1e-05, 'epoch': 2.33}
 47%|████▋     | 173/370 [2:59:20<2:13:09, 40.56s/it] 47%|████▋     | 174/370 [3:00:00<2:11:52, 40.37s/it]                                                     {'loss': 0.4198, 'learning_rate': 1e-05, 'epoch': 2.34}
 47%|████▋     | 174/370 [3:00:00<2:11:52, 40.37s/it] 47%|████▋     | 175/370 [3:00:41<2:11:16, 40.39s/it]                                                     {'loss': 0.4229, 'learning_rate': 1e-05, 'epoch': 2.36}
 47%|████▋     | 175/370 [3:00:41<2:11:16, 40.39s/it] 48%|████▊     | 176/370 [3:01:22<2:11:14, 40.59s/it]                                                     {'loss': 0.4171, 'learning_rate': 1e-05, 'epoch': 2.37}
 48%|████▊     | 176/370 [3:01:22<2:11:14, 40.59s/it] 48%|████▊     | 177/370 [3:02:02<2:10:31, 40.58s/it]                                                     {'loss': 0.4053, 'learning_rate': 1e-05, 'epoch': 2.38}
 48%|████▊     | 177/370 [3:02:02<2:10:31, 40.58s/it] 48%|████▊     | 178/370 [3:02:43<2:09:43, 40.54s/it]                                                     {'loss': 0.4085, 'learning_rate': 1e-05, 'epoch': 2.4}
 48%|████▊     | 178/370 [3:02:43<2:09:43, 40.54s/it] 48%|████▊     | 179/370 [3:03:24<2:09:50, 40.79s/it]                                                     {'loss': 0.4144, 'learning_rate': 1e-05, 'epoch': 2.41}
 48%|████▊     | 179/370 [3:03:24<2:09:50, 40.79s/it] 49%|████▊     | 180/370 [3:04:07<2:10:46, 41.30s/it]                                                     {'loss': 0.4172, 'learning_rate': 1e-05, 'epoch': 2.42}
 49%|████▊     | 180/370 [3:04:07<2:10:46, 41.30s/it]
  0%|          | 0/63 [00:00<?, ?it/s][A
  3%|▎         | 2/63 [00:01<00:31,  1.92it/s][A
  5%|▍         | 3/63 [00:01<00:41,  1.43it/s][A
  6%|▋         | 4/63 [00:03<00:48,  1.23it/s][A
  8%|▊         | 5/63 [00:04<00:51,  1.14it/s][A
 10%|▉         | 6/63 [00:05<00:52,  1.08it/s][A
 11%|█         | 7/63 [00:06<00:53,  1.04it/s][A
 13%|█▎        | 8/63 [00:07<00:53,  1.04it/s][A
 14%|█▍        | 9/63 [00:08<00:54,  1.00s/it][A
 16%|█▌        | 10/63 [00:09<00:53,  1.00s/it][A
 17%|█▋        | 11/63 [00:10<00:52,  1.02s/it][A
 19%|█▉        | 12/63 [00:11<00:51,  1.01s/it][A
 21%|██        | 13/63 [00:12<00:51,  1.03s/it][A
 22%|██▏       | 14/63 [00:13<00:50,  1.03s/it][A
 24%|██▍       | 15/63 [00:14<00:50,  1.05s/it][A
 25%|██▌       | 16/63 [00:15<00:49,  1.05s/it][A
 27%|██▋       | 17/63 [00:16<00:48,  1.06s/it][A
 29%|██▊       | 18/63 [00:17<00:50,  1.11s/it][A
 30%|███       | 19/63 [00:18<00:48,  1.10s/it][A
 32%|███▏      | 20/63 [00:19<00:46,  1.08s/it][A
 33%|███▎      | 21/63 [00:20<00:45,  1.07s/it][A
 35%|███▍      | 22/63 [00:23<00:43,  1.06s/it][A
 37%|███▋      | 23/63 [00:23<00:53,  1.34s/it][A
 38%|███▊      | 24/63 [00:25<00:49,  1.28s/it][A
 40%|███▉      | 25/63 [00:26<00:46,  1.22s/it][A
 41%|████▏     | 26/63 [00:28<00:43,  1.18s/it][A
 43%|████▎     | 27/63 [00:28<00:48,  1.35s/it][A
 44%|████▍     | 28/63 [00:30<00:44,  1.27s/it][A
 46%|████▌     | 29/63 [00:31<00:43,  1.27s/it][A
 48%|████▊     | 30/63 [00:32<00:40,  1.21s/it][A
 49%|████▉     | 31/63 [00:33<00:36,  1.15s/it][A
 51%|█████     | 32/63 [00:34<00:34,  1.11s/it][A
 52%|█████▏    | 33/63 [00:35<00:32,  1.07s/it][A
 54%|█████▍    | 34/63 [00:36<00:32,  1.12s/it][A
 56%|█████▌    | 35/63 [00:37<00:30,  1.07s/it][A
 57%|█████▋    | 36/63 [00:38<00:28,  1.07s/it][A
 59%|█████▊    | 37/63 [00:40<00:28,  1.09s/it][A
 60%|██████    | 38/63 [00:41<00:32,  1.30s/it][A
 62%|██████▏   | 39/63 [00:42<00:29,  1.23s/it][A
 63%|██████▎   | 40/63 [00:43<00:28,  1.23s/it][A
 65%|██████▌   | 41/63 [00:44<00:25,  1.15s/it][A
 67%|██████▋   | 42/63 [00:45<00:23,  1.10s/it][A
 68%|██████▊   | 43/63 [00:46<00:21,  1.07s/it][A
 70%|██████▉   | 44/63 [00:48<00:20,  1.07s/it][A
 71%|███████▏  | 45/63 [00:49<00:22,  1.25s/it][A
 73%|███████▎  | 46/63 [00:50<00:19,  1.17s/it][A
 75%|███████▍  | 47/63 [00:51<00:18,  1.13s/it][A
 76%|███████▌  | 48/63 [00:52<00:16,  1.11s/it][A
 78%|███████▊  | 49/63 [00:53<00:15,  1.08s/it][A
 79%|███████▉  | 50/63 [00:54<00:13,  1.06s/it][A
 81%|████████  | 51/63 [00:55<00:12,  1.05s/it][A
 83%|████████▎ | 52/63 [00:57<00:11,  1.06s/it][A
 84%|████████▍ | 53/63 [00:58<00:11,  1.15s/it][A
 86%|████████▌ | 54/63 [00:59<00:09,  1.10s/it][A
 87%|████████▋ | 55/63 [01:00<00:08,  1.10s/it][A
 89%|████████▉ | 56/63 [01:01<00:07,  1.06s/it][A
 90%|█████████ | 57/63 [01:02<00:06,  1.07s/it][A
 92%|█████████▏| 58/63 [01:03<00:05,  1.11s/it][A
 94%|█████████▎| 59/63 [01:04<00:04,  1.09s/it][A
 95%|█████████▌| 60/63 [01:05<00:03,  1.07s/it][A
 97%|█████████▋| 61/63 [01:06<00:02,  1.07s/it][A
 98%|█████████▊| 62/63 [01:08<00:01,  1.06s/it][A
100%|██████████| 63/63 [01:09<00:00,  1.15s/it][A                                                     
                                               [A{'eval_loss': 0.411865234375, 'eval_runtime': 70.0594, 'eval_samples_per_second': 7.137, 'eval_steps_per_second': 0.899, 'epoch': 2.42}
 49%|████▊     | 180/370 [3:05:17<2:10:46, 41.30s/it]
100%|██████████| 63/63 [01:09<00:00,  1.15s/it][A
                                               [A/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 49%|████▉     | 181/370 [3:13:50<10:42:40, 204.03s/it]                                                       {'loss': 0.4001, 'learning_rate': 1e-05, 'epoch': 2.44}
 49%|████▉     | 181/370 [3:13:50<10:42:40, 204.03s/it] 49%|████▉     | 182/370 [3:14:37<8:10:54, 156.67s/it]                                                       {'loss': 0.4105, 'learning_rate': 1e-05, 'epoch': 2.45}
 49%|████▉     | 182/370 [3:14:37<8:10:54, 156.67s/it] 49%|████▉     | 183/370 [3:15:18<6:20:35, 122.12s/it]                                                      {'loss': 0.4105, 'learning_rate': 1e-05, 'epoch': 2.46}
 49%|████▉     | 183/370 [3:15:18<6:20:35, 122.12s/it] 50%|████▉     | 184/370 [3:16:00<5:04:30, 98.23s/it]                                                      {'loss': 0.4125, 'learning_rate': 1e-05, 'epoch': 2.48}
 50%|████▉     | 184/370 [3:16:01<5:04:30, 98.23s/it] 50%|█████     | 185/370 [3:16:46<4:13:40, 82.27s/it]                                                     {'loss': 0.4087, 'learning_rate': 1e-05, 'epoch': 2.49}
 50%|█████     | 185/370 [3:16:46<4:13:40, 82.27s/it] 50%|█████     | 186/370 [3:17:30<3:37:26, 70.90s/it]                                                     {'loss': 0.4128, 'learning_rate': 1e-05, 'epoch': 2.51}
 50%|█████     | 186/370 [3:17:30<3:37:26, 70.90s/it] 51%|█████     | 187/370 [3:18:31<3:26:51, 67.82s/it]                                                     {'loss': 0.4068, 'learning_rate': 1e-05, 'epoch': 2.52}
 51%|█████     | 187/370 [3:18:31<3:26:51, 67.82s/it] 51%|█████     | 188/370 [3:19:23<3:11:36, 63.17s/it]                                                     {'loss': 0.4131, 'learning_rate': 1e-05, 'epoch': 2.53}
 51%|█████     | 188/370 [3:19:23<3:11:36, 63.17s/it] 51%|█████     | 189/370 [3:20:13<2:58:19, 59.12s/it]                                                     {'loss': 0.4003, 'learning_rate': 1e-05, 'epoch': 2.55}
 51%|█████     | 189/370 [3:20:13<2:58:19, 59.12s/it] 51%|█████▏    | 190/370 [3:20:57<2:44:13, 54.74s/it]                                                     {'loss': 0.4073, 'learning_rate': 1e-05, 'epoch': 2.56}
 51%|█████▏    | 190/370 [3:20:57<2:44:13, 54.74s/it] 52%|█████▏    | 191/370 [3:21:38<2:30:42, 50.52s/it]                                                     {'loss': 0.3987, 'learning_rate': 1e-05, 'epoch': 2.57}
 52%|█████▏    | 191/370 [3:21:38<2:30:42, 50.52s/it] 52%|█████▏    | 192/370 [3:22:19<2:22:00, 47.87s/it]                                                     {'loss': 0.4091, 'learning_rate': 1e-05, 'epoch': 2.59}
 52%|█████▏    | 192/370 [3:22:19<2:22:00, 47.87s/it] 52%|█████▏    | 193/370 [3:23:01<2:15:30, 45.93s/it]                                                     {'loss': 0.3968, 'learning_rate': 1e-05, 'epoch': 2.6}
 52%|█████▏    | 193/370 [3:23:01<2:15:30, 45.93s/it] 52%|█████▏    | 194/370 [3:23:43<2:11:17, 44.76s/it]                                                     {'loss': 0.3956, 'learning_rate': 1e-05, 'epoch': 2.61}
 52%|█████▏    | 194/370 [3:23:43<2:11:17, 44.76s/it] 53%|█████▎    | 195/370 [3:24:26<2:09:02, 44.25s/it]                                                     {'loss': 0.4151, 'learning_rate': 1e-05, 'epoch': 2.63}
 53%|█████▎    | 195/370 [3:24:26<2:09:02, 44.25s/it] 53%|█████▎    | 196/370 [3:25:07<2:05:13, 43.18s/it]                                                     {'loss': 0.4057, 'learning_rate': 1e-05, 'epoch': 2.64}
 53%|█████▎    | 196/370 [3:25:07<2:05:13, 43.18s/it] 53%|█████▎    | 197/370 [3:25:48<2:02:50, 42.60s/it]                                                     {'loss': 0.4066, 'learning_rate': 1e-05, 'epoch': 2.65}
 53%|█████▎    | 197/370 [3:25:48<2:02:50, 42.60s/it] 54%|█████▎    | 198/370 [3:26:29<2:00:48, 42.14s/it]                                                     {'loss': 0.3957, 'learning_rate': 1e-05, 'epoch': 2.67}
 54%|█████▎    | 198/370 [3:26:29<2:00:48, 42.14s/it] 54%|█████▍    | 199/370 [3:27:11<2:00:11, 42.17s/it]                                                     {'loss': 0.4112, 'learning_rate': 1e-05, 'epoch': 2.68}
 54%|█████▍    | 199/370 [3:27:11<2:00:11, 42.17s/it] 54%|█████▍    | 200/370 [3:27:51<1:57:39, 41.53s/it]                                                     {'loss': 0.4224, 'learning_rate': 1e-05, 'epoch': 2.69}
 54%|█████▍    | 200/370 [3:27:51<1:57:39, 41.53s/it] 54%|█████▍    | 201/370 [3:28:32<1:56:26, 41.34s/it]                                                     {'loss': 0.416, 'learning_rate': 1e-05, 'epoch': 2.71}
 54%|█████▍    | 201/370 [3:28:32<1:56:26, 41.34s/it] 55%|█████▍    | 202/370 [3:29:14<1:56:17, 41.53s/it]                                                     {'loss': 0.3969, 'learning_rate': 1e-05, 'epoch': 2.72}
 55%|█████▍    | 202/370 [3:29:14<1:56:17, 41.53s/it] 55%|█████▍    | 203/370 [3:29:56<1:55:45, 41.59s/it]                                                     {'loss': 0.4035, 'learning_rate': 1e-05, 'epoch': 2.73}
 55%|█████▍    | 203/370 [3:29:56<1:55:45, 41.59s/it] 55%|█████▌    | 204/370 [3:30:37<1:55:07, 41.61s/it]                                                     {'loss': 0.3985, 'learning_rate': 1e-05, 'epoch': 2.75}
 55%|█████▌    | 204/370 [3:30:38<1:55:07, 41.61s/it] 55%|█████▌    | 205/370 [3:31:20<1:55:04, 41.85s/it]                                                     {'loss': 0.3949, 'learning_rate': 1e-05, 'epoch': 2.76}
 55%|█████▌    | 205/370 [3:31:20<1:55:04, 41.85s/it] 56%|█████▌    | 206/370 [3:32:00<1:53:15, 41.44s/it]                                                     {'loss': 0.4004, 'learning_rate': 1e-05, 'epoch': 2.77}
 56%|█████▌    | 206/370 [3:32:00<1:53:15, 41.44s/it] 56%|█████▌    | 207/370 [3:32:42<1:53:08, 41.65s/it]                                                     {'loss': 0.4115, 'learning_rate': 1e-05, 'epoch': 2.79}
 56%|█████▌    | 207/370 [3:32:42<1:53:08, 41.65s/it] 56%|█████▌    | 208/370 [3:33:26<1:53:42, 42.11s/it]                                                     {'loss': 0.4, 'learning_rate': 1e-05, 'epoch': 2.8}
 56%|█████▌    | 208/370 [3:33:26<1:53:42, 42.11s/it] 56%|█████▋    | 209/370 [3:34:06<1:51:42, 41.63s/it]                                                     {'loss': 0.3938, 'learning_rate': 1e-05, 'epoch': 2.81}
 56%|█████▋    | 209/370 [3:34:06<1:51:42, 41.63s/it] 57%|█████▋    | 210/370 [3:34:47<1:50:06, 41.29s/it]                                                     {'loss': 0.4017, 'learning_rate': 1e-05, 'epoch': 2.83}
 57%|█████▋    | 210/370 [3:34:47<1:50:06, 41.29s/it] 57%|█████▋    | 211/370 [3:35:28<1:49:26, 41.30s/it]                                                     {'loss': 0.3884, 'learning_rate': 1e-05, 'epoch': 2.84}
 57%|█████▋    | 211/370 [3:35:28<1:49:26, 41.30s/it] 57%|█████▋    | 212/370 [3:36:11<1:50:26, 41.94s/it]                                                     {'loss': 0.4041, 'learning_rate': 1e-05, 'epoch': 2.86}
 57%|█████▋    | 212/370 [3:36:11<1:50:26, 41.94s/it] 58%|█████▊    | 213/370 [3:36:52<1:48:43, 41.55s/it]                                                     {'loss': 0.4045, 'learning_rate': 1e-05, 'epoch': 2.87}
 58%|█████▊    | 213/370 [3:36:52<1:48:43, 41.55s/it] 58%|█████▊    | 214/370 [3:37:34<1:48:16, 41.64s/it]                                                     {'loss': 0.403, 'learning_rate': 1e-05, 'epoch': 2.88}
 58%|█████▊    | 214/370 [3:37:34<1:48:16, 41.64s/it] 58%|█████▊    | 215/370 [3:38:18<1:49:35, 42.42s/it]                                                     {'loss': 0.4015, 'learning_rate': 1e-05, 'epoch': 2.9}
 58%|█████▊    | 215/370 [3:38:18<1:49:35, 42.42s/it] 58%|█████▊    | 216/370 [3:39:00<1:48:38, 42.32s/it]                                                     {'loss': 0.388, 'learning_rate': 1e-05, 'epoch': 2.91}
 58%|█████▊    | 216/370 [3:39:00<1:48:38, 42.32s/it] 59%|█████▊    | 217/370 [3:39:42<1:47:26, 42.13s/it]                                                     {'loss': 0.3978, 'learning_rate': 1e-05, 'epoch': 2.92}
 59%|█████▊    | 217/370 [3:39:42<1:47:26, 42.13s/it] 59%|█████▉    | 218/370 [3:40:24<1:46:37, 42.09s/it]                                                     {'loss': 0.3876, 'learning_rate': 1e-05, 'epoch': 2.94}
 59%|█████▉    | 218/370 [3:40:24<1:46:37, 42.09s/it] 59%|█████▉    | 219/370 [3:41:08<1:47:20, 42.65s/it]                                                     {'loss': 0.3992, 'learning_rate': 1e-05, 'epoch': 2.95}
 59%|█████▉    | 219/370 [3:41:08<1:47:20, 42.65s/it] 59%|█████▉    | 220/370 [3:41:48<1:44:59, 41.99s/it]                                                     {'loss': 0.3841, 'learning_rate': 1e-05, 'epoch': 2.96}
 59%|█████▉    | 220/370 [3:41:48<1:44:59, 41.99s/it] 60%|█████▉    | 221/370 [3:42:30<1:43:54, 41.84s/it]                                                     {'loss': 0.4044, 'learning_rate': 1e-05, 'epoch': 2.98}
 60%|█████▉    | 221/370 [3:42:30<1:43:54, 41.84s/it] 60%|██████    | 222/370 [3:43:11<1:42:22, 41.50s/it]                                                     {'loss': 0.3935, 'learning_rate': 1e-05, 'epoch': 2.99}
 60%|██████    | 222/370 [3:43:11<1:42:22, 41.50s/it] 60%|██████    | 223/370 [3:43:54<1:43:01, 42.05s/it]                                                     {'loss': 0.3991, 'learning_rate': 1e-05, 'epoch': 3.0}
 60%|██████    | 223/370 [3:43:54<1:43:01, 42.05s/it] 61%|██████    | 224/370 [3:44:34<1:41:17, 41.63s/it]                                                     {'loss': 0.3916, 'learning_rate': 1e-05, 'epoch': 3.02}
 61%|██████    | 224/370 [3:44:35<1:41:17, 41.63s/it] 61%|██████    | 225/370 [3:45:15<1:39:42, 41.26s/it]                                                     {'loss': 0.3947, 'learning_rate': 1e-05, 'epoch': 3.03}
 61%|██████    | 225/370 [3:45:15<1:39:42, 41.26s/it] 61%|██████    | 226/370 [3:45:55<1:38:16, 40.95s/it]                                                     {'loss': 0.4006, 'learning_rate': 1e-05, 'epoch': 3.04}
 61%|██████    | 226/370 [3:45:55<1:38:16, 40.95s/it] 61%|██████▏   | 227/370 [3:46:35<1:36:54, 40.66s/it]                                                     {'loss': 0.3738, 'learning_rate': 1e-05, 'epoch': 3.06}
 61%|██████▏   | 227/370 [3:46:35<1:36:54, 40.66s/it] 62%|██████▏   | 228/370 [3:47:16<1:36:19, 40.70s/it]                                                     {'loss': 0.3957, 'learning_rate': 1e-05, 'epoch': 3.07}
 62%|██████▏   | 228/370 [3:47:16<1:36:19, 40.70s/it] 62%|██████▏   | 229/370 [3:48:01<1:38:28, 41.91s/it]                                                     {'loss': 0.3859, 'learning_rate': 1e-05, 'epoch': 3.08}
 62%|██████▏   | 229/370 [3:48:01<1:38:28, 41.91s/it] 62%|██████▏   | 230/370 [3:48:41<1:36:33, 41.38s/it]                                                     {'loss': 0.386, 'learning_rate': 1e-05, 'epoch': 3.1}
 62%|██████▏   | 230/370 [3:48:41<1:36:33, 41.38s/it] 62%|██████▏   | 231/370 [3:49:21<1:35:08, 41.07s/it]                                                     {'loss': 0.3928, 'learning_rate': 1e-05, 'epoch': 3.11}
 62%|██████▏   | 231/370 [3:49:21<1:35:08, 41.07s/it] 63%|██████▎   | 232/370 [3:50:01<1:33:49, 40.79s/it]                                                     {'loss': 0.3774, 'learning_rate': 1e-05, 'epoch': 3.12}
 63%|██████▎   | 232/370 [3:50:01<1:33:49, 40.79s/it] 63%|██████▎   | 233/370 [3:50:42<1:32:51, 40.67s/it]                                                     {'loss': 0.3952, 'learning_rate': 1e-05, 'epoch': 3.14}
 63%|██████▎   | 233/370 [3:50:42<1:32:51, 40.67s/it] 63%|██████▎   | 234/370 [3:51:22<1:31:43, 40.47s/it]                                                     {'loss': 0.4071, 'learning_rate': 1e-05, 'epoch': 3.15}
 63%|██████▎   | 234/370 [3:51:22<1:31:43, 40.47s/it] 64%|██████▎   | 235/370 [3:52:02<1:30:42, 40.31s/it]                                                     {'loss': 0.3848, 'learning_rate': 1e-05, 'epoch': 3.16}
 64%|██████▎   | 235/370 [3:52:02<1:30:42, 40.31s/it] 64%|██████▍   | 236/370 [3:52:42<1:30:20, 40.45s/it]                                                     {'loss': 0.395, 'learning_rate': 1e-05, 'epoch': 3.18}
 64%|██████▍   | 236/370 [3:52:42<1:30:20, 40.45s/it] 64%|██████▍   | 237/370 [3:53:23<1:29:37, 40.43s/it]                                                     {'loss': 0.3876, 'learning_rate': 1e-05, 'epoch': 3.19}
 64%|██████▍   | 237/370 [3:53:23<1:29:37, 40.43s/it] 64%|██████▍   | 238/370 [3:54:03<1:29:00, 40.46s/it]                                                     {'loss': 0.3871, 'learning_rate': 1e-05, 'epoch': 3.21}
 64%|██████▍   | 238/370 [3:54:03<1:29:00, 40.46s/it] 65%|██████▍   | 239/370 [3:54:44<1:28:22, 40.47s/it]                                                     {'loss': 0.3739, 'learning_rate': 1e-05, 'epoch': 3.22}
 65%|██████▍   | 239/370 [3:54:44<1:28:22, 40.47s/it] 65%|██████▍   | 240/370 [3:55:24<1:27:34, 40.42s/it]                                                     {'loss': 0.3884, 'learning_rate': 1e-05, 'epoch': 3.23}
 65%|██████▍   | 240/370 [3:55:24<1:27:34, 40.42s/it]
  0%|          | 0/63 [00:00<?, ?it/s][A
  3%|▎         | 2/63 [00:01<00:34,  1.77it/s][A
  5%|▍         | 3/63 [00:02<00:45,  1.32it/s][A
  6%|▋         | 4/63 [00:03<00:50,  1.16it/s][A
  8%|▊         | 5/63 [00:04<00:53,  1.09it/s][A
 10%|▉         | 6/63 [00:05<00:54,  1.05it/s][A
 11%|█         | 7/63 [00:06<00:53,  1.04it/s][A
 13%|█▎        | 8/63 [00:07<00:53,  1.04it/s][A
 14%|█▍        | 9/63 [00:08<00:53,  1.01it/s][A
 16%|█▌        | 10/63 [00:09<00:53,  1.00s/it][A
 17%|█▋        | 11/63 [00:10<00:52,  1.01s/it][A
 19%|█▉        | 12/63 [00:11<00:52,  1.02s/it][A
 21%|██        | 13/63 [00:12<00:51,  1.02s/it][A
 22%|██▏       | 14/63 [00:13<00:50,  1.03s/it][A
 24%|██▍       | 15/63 [00:14<00:49,  1.03s/it][A
 25%|██▌       | 16/63 [00:15<00:48,  1.02s/it][A
 27%|██▋       | 17/63 [00:16<00:50,  1.09s/it][A
 29%|██▊       | 18/63 [00:17<00:48,  1.07s/it][A
 30%|███       | 19/63 [00:18<00:47,  1.07s/it][A
 32%|███▏      | 20/63 [00:19<00:46,  1.08s/it][A
 33%|███▎      | 21/63 [00:21<00:44,  1.06s/it][A
 35%|███▍      | 22/63 [00:22<00:44,  1.10s/it][A
 37%|███▋      | 23/63 [00:23<00:42,  1.06s/it][A
 38%|███▊      | 24/63 [00:24<00:41,  1.07s/it][A
 40%|███▉      | 25/63 [00:25<00:39,  1.05s/it][A
 41%|████▏     | 26/63 [00:26<00:39,  1.07s/it][A
 43%|████▎     | 27/63 [00:27<00:38,  1.07s/it][A
 44%|████▍     | 28/63 [00:28<00:37,  1.07s/it][A
 46%|████▌     | 29/63 [00:29<00:35,  1.06s/it][A
 48%|████▊     | 30/63 [00:30<00:34,  1.05s/it][A
 49%|████▉     | 31/63 [00:31<00:33,  1.03s/it][A
 51%|█████     | 32/63 [00:32<00:32,  1.04s/it][A
 52%|█████▏    | 33/63 [00:33<00:32,  1.07s/it][A
 54%|█████▍    | 34/63 [00:34<00:30,  1.06s/it][A
 56%|█████▌    | 35/63 [00:35<00:29,  1.05s/it][A
 57%|█████▋    | 36/63 [00:36<00:28,  1.05s/it][A
 59%|█████▊    | 37/63 [00:37<00:27,  1.05s/it][A
 60%|██████    | 38/63 [00:38<00:26,  1.05s/it][A
 62%|██████▏   | 39/63 [00:39<00:24,  1.04s/it][A
 63%|██████▎   | 40/63 [00:40<00:24,  1.05s/it][A
 65%|██████▌   | 41/63 [00:41<00:22,  1.03s/it][A
 67%|██████▋   | 42/63 [00:42<00:21,  1.01s/it][A
 68%|██████▊   | 43/63 [00:43<00:20,  1.02s/it][A
 70%|██████▉   | 44/63 [00:44<00:19,  1.00s/it][A
 71%|███████▏  | 45/63 [00:45<00:18,  1.01s/it][A
 73%|███████▎  | 46/63 [00:46<00:16,  1.00it/s][A
 75%|███████▍  | 47/63 [00:47<00:15,  1.00it/s][A
 76%|███████▌  | 48/63 [00:49<00:15,  1.02s/it][A
 78%|███████▊  | 49/63 [00:50<00:14,  1.04s/it][A
 79%|███████▉  | 50/63 [00:51<00:13,  1.05s/it][A
 81%|████████  | 51/63 [00:52<00:12,  1.03s/it][A
 83%|████████▎ | 52/63 [00:53<00:11,  1.03s/it][A
 84%|████████▍ | 53/63 [00:54<00:10,  1.03s/it][A
 86%|████████▌ | 54/63 [00:55<00:09,  1.03s/it][A
 87%|████████▋ | 55/63 [00:56<00:08,  1.01s/it][A
 89%|████████▉ | 56/63 [00:57<00:07,  1.01s/it][A
 90%|█████████ | 57/63 [00:58<00:06,  1.01s/it][A
 92%|█████████▏| 58/63 [00:59<00:05,  1.03s/it][A
 94%|█████████▎| 59/63 [01:00<00:04,  1.03s/it][A
 95%|█████████▌| 60/63 [01:01<00:03,  1.05s/it][A
 97%|█████████▋| 61/63 [01:02<00:02,  1.04s/it][A
 98%|█████████▊| 62/63 [01:03<00:01,  1.04s/it][A
100%|██████████| 63/63 [01:04<00:00,  1.04s/it][A                                                     
                                               [A{'eval_loss': 0.389404296875, 'eval_runtime': 65.5304, 'eval_samples_per_second': 7.63, 'eval_steps_per_second': 0.961, 'epoch': 3.23}
 65%|██████▍   | 240/370 [3:56:30<1:27:34, 40.42s/it]
100%|██████████| 63/63 [01:04<00:00,  1.04s/it][A
                                               [A/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 65%|██████▌   | 241/370 [4:03:15<6:04:18, 169.45s/it]                                                      {'loss': 0.3834, 'learning_rate': 1e-05, 'epoch': 3.25}
 65%|██████▌   | 241/370 [4:03:15<6:04:18, 169.45s/it] 65%|██████▌   | 242/370 [4:03:56<4:39:27, 131.00s/it]                                                      {'loss': 0.3902, 'learning_rate': 1e-05, 'epoch': 3.26}
 65%|██████▌   | 242/370 [4:03:56<4:39:27, 131.00s/it] 66%|██████▌   | 243/370 [4:04:38<3:40:53, 104.36s/it]                                                      {'loss': 0.386, 'learning_rate': 1e-05, 'epoch': 3.27}
 66%|██████▌   | 243/370 [4:04:38<3:40:53, 104.36s/it] 66%|██████▌   | 244/370 [4:05:19<2:58:58, 85.23s/it]                                                      {'loss': 0.3934, 'learning_rate': 1e-05, 'epoch': 3.29}
 66%|██████▌   | 244/370 [4:05:19<2:58:58, 85.23s/it] 66%|██████▌   | 245/370 [4:05:59<2:29:25, 71.73s/it]                                                     {'loss': 0.3929, 'learning_rate': 1e-05, 'epoch': 3.3}
 66%|██████▌   | 245/370 [4:05:59<2:29:25, 71.73s/it] 66%|██████▋   | 246/370 [4:06:39<2:08:47, 62.32s/it]                                                     {'loss': 0.3784, 'learning_rate': 1e-05, 'epoch': 3.31}
 66%|██████▋   | 246/370 [4:06:39<2:08:47, 62.32s/it] 67%|██████▋   | 247/370 [4:07:29<1:59:43, 58.40s/it]                                                     {'loss': 0.3793, 'learning_rate': 1e-05, 'epoch': 3.33}
 67%|██████▋   | 247/370 [4:07:29<1:59:43, 58.40s/it] 67%|██████▋   | 248/370 [4:08:09<1:47:45, 53.00s/it]                                                     {'loss': 0.4002, 'learning_rate': 1e-05, 'epoch': 3.34}
 67%|██████▋   | 248/370 [4:08:09<1:47:45, 53.00s/it] 67%|██████▋   | 249/370 [4:08:50<1:39:55, 49.55s/it]                                                     {'loss': 0.3881, 'learning_rate': 1e-05, 'epoch': 3.35}
 67%|██████▋   | 249/370 [4:08:50<1:39:55, 49.55s/it] 68%|██████▊   | 250/370 [4:09:31<1:33:38, 46.82s/it]                                                     {'loss': 0.3736, 'learning_rate': 1e-05, 'epoch': 3.37}
 68%|██████▊   | 250/370 [4:09:31<1:33:38, 46.82s/it] 68%|██████▊   | 251/370 [4:10:11<1:28:56, 44.85s/it]                                                     {'loss': 0.3836, 'learning_rate': 1e-05, 'epoch': 3.38}
 68%|██████▊   | 251/370 [4:10:11<1:28:56, 44.85s/it] 68%|██████▊   | 252/370 [4:10:51<1:25:17, 43.37s/it]                                                     {'loss': 0.3758, 'learning_rate': 1e-05, 'epoch': 3.39}
 68%|██████▊   | 252/370 [4:10:51<1:25:17, 43.37s/it] 68%|██████▊   | 253/370 [4:11:33<1:23:47, 42.97s/it]                                                     {'loss': 0.3989, 'learning_rate': 1e-05, 'epoch': 3.41}
 68%|██████▊   | 253/370 [4:11:33<1:23:47, 42.97s/it] 69%|██████▊   | 254/370 [4:12:14<1:21:59, 42.41s/it]                                                     {'loss': 0.4028, 'learning_rate': 1e-05, 'epoch': 3.42}
 69%|██████▊   | 254/370 [4:12:14<1:21:59, 42.41s/it] 69%|██████▉   | 255/370 [4:12:54<1:19:56, 41.71s/it]                                                     {'loss': 0.395, 'learning_rate': 1e-05, 'epoch': 3.43}
 69%|██████▉   | 255/370 [4:12:54<1:19:56, 41.71s/it] 69%|██████▉   | 256/370 [4:13:36<1:19:07, 41.64s/it]                                                     {'loss': 0.3901, 'learning_rate': 1e-05, 'epoch': 3.45}
 69%|██████▉   | 256/370 [4:13:36<1:19:07, 41.64s/it] 69%|██████▉   | 257/370 [4:14:16<1:17:45, 41.28s/it]                                                     {'loss': 0.3861, 'learning_rate': 1e-05, 'epoch': 3.46}
 69%|██████▉   | 257/370 [4:14:16<1:17:45, 41.28s/it] 70%|██████▉   | 258/370 [4:14:57<1:16:40, 41.08s/it]                                                     {'loss': 0.3737, 'learning_rate': 1e-05, 'epoch': 3.47}
 70%|██████▉   | 258/370 [4:14:57<1:16:40, 41.08s/it] 70%|███████   | 259/370 [4:15:39<1:16:22, 41.29s/it]                                                     {'loss': 0.3815, 'learning_rate': 1e-05, 'epoch': 3.49}
 70%|███████   | 259/370 [4:15:39<1:16:22, 41.29s/it] 70%|███████   | 260/370 [4:16:20<1:15:38, 41.26s/it]                                                     {'loss': 0.3826, 'learning_rate': 1e-05, 'epoch': 3.5}
 70%|███████   | 260/370 [4:16:20<1:15:38, 41.26s/it] 71%|███████   | 261/370 [4:17:00<1:14:35, 41.06s/it]                                                     {'loss': 0.3767, 'learning_rate': 1e-05, 'epoch': 3.52}
 71%|███████   | 261/370 [4:17:00<1:14:35, 41.06s/it] 71%|███████   | 262/370 [4:17:41<1:13:55, 41.07s/it]                                                     {'loss': 0.3706, 'learning_rate': 1e-05, 'epoch': 3.53}
 71%|███████   | 262/370 [4:17:41<1:13:55, 41.07s/it] 71%|███████   | 263/370 [4:18:22<1:13:01, 40.95s/it]                                                     {'loss': 0.3839, 'learning_rate': 1e-05, 'epoch': 3.54}
 71%|███████   | 263/370 [4:18:22<1:13:01, 40.95s/it] 71%|███████▏  | 264/370 [4:19:03<1:12:13, 40.88s/it]                                                     {'loss': 0.391, 'learning_rate': 1e-05, 'epoch': 3.56}
 71%|███████▏  | 264/370 [4:19:03<1:12:13, 40.88s/it] 72%|███████▏  | 265/370 [4:19:43<1:11:23, 40.79s/it]                                                     {'loss': 0.3759, 'learning_rate': 1e-05, 'epoch': 3.57}
 72%|███████▏  | 265/370 [4:19:43<1:11:23, 40.79s/it] 72%|███████▏  | 266/370 [4:20:24<1:10:26, 40.64s/it]                                                     {'loss': 0.3876, 'learning_rate': 1e-05, 'epoch': 3.58}
 72%|███████▏  | 266/370 [4:20:24<1:10:26, 40.64s/it] 72%|███████▏  | 267/370 [4:21:04<1:09:29, 40.48s/it]                                                     {'loss': 0.3897, 'learning_rate': 1e-05, 'epoch': 3.6}
 72%|███████▏  | 267/370 [4:21:04<1:09:29, 40.48s/it] 72%|███████▏  | 268/370 [4:21:44<1:08:50, 40.50s/it]                                                     {'loss': 0.3919, 'learning_rate': 1e-05, 'epoch': 3.61}
 72%|███████▏  | 268/370 [4:21:44<1:08:50, 40.50s/it] 73%|███████▎  | 269/370 [4:22:25<1:08:27, 40.67s/it]                                                     {'loss': 0.37, 'learning_rate': 1e-05, 'epoch': 3.62}
 73%|███████▎  | 269/370 [4:22:25<1:08:27, 40.67s/it] 73%|███████▎  | 270/370 [4:23:06<1:07:40, 40.60s/it]                                                     {'loss': 0.3779, 'learning_rate': 1e-05, 'epoch': 3.64}
 73%|███████▎  | 270/370 [4:23:06<1:07:40, 40.60s/it] 73%|███████▎  | 271/370 [4:23:49<1:08:14, 41.36s/it]                                                     {'loss': 0.3938, 'learning_rate': 1e-05, 'epoch': 3.65}
 73%|███████▎  | 271/370 [4:23:49<1:08:14, 41.36s/it] 74%|███████▎  | 272/370 [4:24:30<1:07:24, 41.27s/it]                                                     {'loss': 0.3859, 'learning_rate': 1e-05, 'epoch': 3.66}
 74%|███████▎  | 272/370 [4:24:30<1:07:24, 41.27s/it] 74%|███████▍  | 273/370 [4:25:11<1:06:21, 41.04s/it]                                                     {'loss': 0.3656, 'learning_rate': 1e-05, 'epoch': 3.68}
 74%|███████▍  | 273/370 [4:25:11<1:06:21, 41.04s/it] 74%|███████▍  | 274/370 [4:25:51<1:05:17, 40.80s/it]                                                     {'loss': 0.3822, 'learning_rate': 1e-05, 'epoch': 3.69}
 74%|███████▍  | 274/370 [4:25:51<1:05:17, 40.80s/it] 74%|███████▍  | 275/370 [4:26:31<1:04:30, 40.74s/it]                                                     {'loss': 0.3865, 'learning_rate': 1e-05, 'epoch': 3.7}
 74%|███████▍  | 275/370 [4:26:31<1:04:30, 40.74s/it] 75%|███████▍  | 276/370 [4:27:13<1:04:02, 40.88s/it]                                                     {'loss': 0.3719, 'learning_rate': 1e-05, 'epoch': 3.72}
 75%|███████▍  | 276/370 [4:27:13<1:04:02, 40.88s/it] 75%|███████▍  | 277/370 [4:27:53<1:03:21, 40.87s/it]                                                     {'loss': 0.383, 'learning_rate': 1e-05, 'epoch': 3.73}
 75%|███████▍  | 277/370 [4:27:53<1:03:21, 40.87s/it] 75%|███████▌  | 278/370 [4:28:35<1:02:48, 40.96s/it]                                                     {'loss': 0.3768, 'learning_rate': 1e-05, 'epoch': 3.74}
 75%|███████▌  | 278/370 [4:28:35<1:02:48, 40.96s/it] 75%|███████▌  | 279/370 [4:29:15<1:02:04, 40.93s/it]                                                     {'loss': 0.3669, 'learning_rate': 1e-05, 'epoch': 3.76}
 75%|███████▌  | 279/370 [4:29:16<1:02:04, 40.93s/it] 76%|███████▌  | 280/370 [4:29:56<1:01:12, 40.81s/it]                                                     {'loss': 0.3807, 'learning_rate': 1e-05, 'epoch': 3.77}
 76%|███████▌  | 280/370 [4:29:56<1:01:12, 40.81s/it] 76%|███████▌  | 281/370 [4:30:37<1:00:45, 40.96s/it]                                                     {'loss': 0.3899, 'learning_rate': 1e-05, 'epoch': 3.78}
 76%|███████▌  | 281/370 [4:30:37<1:00:45, 40.96s/it] 76%|███████▌  | 282/370 [4:31:17<59:43, 40.73s/it]                                                     {'loss': 0.372, 'learning_rate': 1e-05, 'epoch': 3.8}
 76%|███████▌  | 282/370 [4:31:18<59:43, 40.73s/it] 76%|███████▋  | 283/370 [4:31:59<59:10, 40.81s/it]                                                   {'loss': 0.3767, 'learning_rate': 1e-05, 'epoch': 3.81}
 76%|███████▋  | 283/370 [4:31:59<59:10, 40.81s/it] 77%|███████▋  | 284/370 [4:32:39<58:26, 40.77s/it]                                                   {'loss': 0.381, 'learning_rate': 1e-05, 'epoch': 3.82}
 77%|███████▋  | 284/370 [4:32:39<58:26, 40.77s/it] 77%|███████▋  | 285/370 [4:33:21<58:06, 41.02s/it]                                                   {'loss': 0.3977, 'learning_rate': 1e-05, 'epoch': 3.84}
 77%|███████▋  | 285/370 [4:33:21<58:06, 41.02s/it] 77%|███████▋  | 286/370 [4:34:02<57:25, 41.01s/it]                                                   {'loss': 0.3765, 'learning_rate': 1e-05, 'epoch': 3.85}
 77%|███████▋  | 286/370 [4:34:02<57:25, 41.01s/it] 78%|███████▊  | 287/370 [4:34:43<56:52, 41.11s/it]                                                   {'loss': 0.3809, 'learning_rate': 1e-05, 'epoch': 3.87}
 78%|███████▊  | 287/370 [4:34:43<56:52, 41.11s/it] 78%|███████▊  | 288/370 [4:35:24<55:58, 40.96s/it]                                                   {'loss': 0.384, 'learning_rate': 1e-05, 'epoch': 3.88}
 78%|███████▊  | 288/370 [4:35:24<55:58, 40.96s/it] 78%|███████▊  | 289/370 [4:36:04<55:12, 40.90s/it]                                                   {'loss': 0.3822, 'learning_rate': 1e-05, 'epoch': 3.89}
 78%|███████▊  | 289/370 [4:36:05<55:12, 40.90s/it] 78%|███████▊  | 290/370 [4:36:47<55:06, 41.33s/it]                                                   {'loss': 0.3728, 'learning_rate': 1e-05, 'epoch': 3.91}
 78%|███████▊  | 290/370 [4:36:47<55:06, 41.33s/it] 79%|███████▊  | 291/370 [4:37:29<54:51, 41.67s/it]                                                   {'loss': 0.3774, 'learning_rate': 1e-05, 'epoch': 3.92}
 79%|███████▊  | 291/370 [4:37:29<54:51, 41.67s/it] 79%|███████▉  | 292/370 [4:38:11<54:03, 41.59s/it]                                                   {'loss': 0.3706, 'learning_rate': 1e-05, 'epoch': 3.93}
 79%|███████▉  | 292/370 [4:38:11<54:03, 41.59s/it] 79%|███████▉  | 293/370 [4:38:52<53:06, 41.38s/it]                                                   {'loss': 0.3754, 'learning_rate': 1e-05, 'epoch': 3.95}
 79%|███████▉  | 293/370 [4:38:52<53:06, 41.38s/it] 79%|███████▉  | 294/370 [4:39:33<52:20, 41.32s/it]                                                   {'loss': 0.381, 'learning_rate': 1e-05, 'epoch': 3.96}
 79%|███████▉  | 294/370 [4:39:33<52:20, 41.32s/it] 80%|███████▉  | 295/370 [4:40:13<51:17, 41.03s/it]                                                   {'loss': 0.3846, 'learning_rate': 1e-05, 'epoch': 3.97}
 80%|███████▉  | 295/370 [4:40:13<51:17, 41.03s/it] 80%|████████  | 296/370 [4:40:53<50:19, 40.81s/it]                                                   {'loss': 0.3717, 'learning_rate': 1e-05, 'epoch': 3.99}
 80%|████████  | 296/370 [4:40:53<50:19, 40.81s/it] 80%|████████  | 297/370 [4:41:33<49:21, 40.57s/it]                                                   {'loss': 0.3656, 'learning_rate': 1e-05, 'epoch': 4.0}
 80%|████████  | 297/370 [4:41:33<49:21, 40.57s/it] 81%|████████  | 298/370 [4:42:14<48:47, 40.65s/it]                                                   {'loss': 0.3731, 'learning_rate': 1e-05, 'epoch': 4.01}
 81%|████████  | 298/370 [4:42:14<48:47, 40.65s/it] 81%|████████  | 299/370 [4:42:54<47:52, 40.46s/it]                                                   {'loss': 0.3564, 'learning_rate': 1e-05, 'epoch': 4.03}
 81%|████████  | 299/370 [4:42:54<47:52, 40.46s/it] 81%|████████  | 300/370 [4:43:35<47:08, 40.41s/it]                                                   {'loss': 0.3706, 'learning_rate': 1e-05, 'epoch': 4.04}
 81%|████████  | 300/370 [4:43:35<47:08, 40.41s/it]
  0%|          | 0/63 [00:00<?, ?it/s][A
  3%|▎         | 2/63 [00:01<00:31,  1.94it/s][A
  5%|▍         | 3/63 [00:01<00:41,  1.44it/s][A
  6%|▋         | 4/63 [00:03<00:48,  1.21it/s][A
  8%|▊         | 5/63 [00:03<00:50,  1.14it/s][A
 10%|▉         | 6/63 [00:05<00:53,  1.06it/s][A
 11%|█         | 7/63 [00:06<00:53,  1.05it/s][A
 13%|█▎        | 8/63 [00:07<00:52,  1.05it/s][A
 14%|█▍        | 9/63 [00:07<00:51,  1.04it/s][A
 16%|█▌        | 10/63 [00:08<00:51,  1.04it/s][A
 17%|█▋        | 11/63 [00:09<00:50,  1.04it/s][A
 19%|█▉        | 12/63 [00:10<00:50,  1.01it/s][A
 21%|██        | 13/63 [00:11<00:49,  1.01it/s][A
 22%|██▏       | 14/63 [00:12<00:48,  1.02it/s][A
 24%|██▍       | 15/63 [00:13<00:47,  1.00it/s][A
 25%|██▌       | 16/63 [00:14<00:47,  1.01s/it][A
 27%|██▋       | 17/63 [00:16<00:46,  1.01s/it][A
 29%|██▊       | 18/63 [00:17<00:45,  1.00s/it][A
 30%|███       | 19/63 [00:18<00:45,  1.02s/it][A
 32%|███▏      | 20/63 [00:19<00:44,  1.03s/it][A
 33%|███▎      | 21/63 [00:20<00:43,  1.04s/it][A
 35%|███▍      | 22/63 [00:21<00:42,  1.03s/it][A
 37%|███▋      | 23/63 [00:22<00:42,  1.06s/it][A
 38%|███▊      | 24/63 [00:23<00:41,  1.05s/it][A
 40%|███▉      | 25/63 [00:24<00:39,  1.04s/it][A
 41%|████▏     | 26/63 [00:25<00:38,  1.03s/it][A
 43%|████▎     | 27/63 [00:26<00:36,  1.03s/it][A
 44%|████▍     | 28/63 [00:27<00:35,  1.03s/it][A
 46%|████▌     | 29/63 [00:28<00:34,  1.02s/it][A
 48%|████▊     | 30/63 [00:29<00:33,  1.01s/it][A
 49%|████▉     | 31/63 [00:30<00:31,  1.00it/s][A
 51%|█████     | 32/63 [00:31<00:31,  1.00s/it][A
 52%|█████▏    | 33/63 [00:32<00:30,  1.02s/it][A
 54%|█████▍    | 34/63 [00:33<00:29,  1.02s/it][A
 56%|█████▌    | 35/63 [00:34<00:28,  1.03s/it][A
 57%|█████▋    | 36/63 [00:35<00:28,  1.04s/it][A
 59%|█████▊    | 37/63 [00:36<00:27,  1.07s/it][A
 60%|██████    | 38/63 [00:37<00:25,  1.04s/it][A
 62%|██████▏   | 39/63 [00:38<00:25,  1.08s/it][A
 63%|██████▎   | 40/63 [00:39<00:24,  1.06s/it][A
 65%|██████▌   | 41/63 [00:40<00:23,  1.05s/it][A
 67%|██████▋   | 42/63 [00:41<00:21,  1.04s/it][A
 68%|██████▊   | 43/63 [00:42<00:20,  1.05s/it][A
 70%|██████▉   | 44/63 [00:44<00:19,  1.05s/it][A
 71%|███████▏  | 45/63 [00:45<00:18,  1.03s/it][A
 73%|███████▎  | 46/63 [00:46<00:17,  1.02s/it][A
 75%|███████▍  | 47/63 [00:47<00:16,  1.05s/it][A
 76%|███████▌  | 48/63 [00:48<00:15,  1.03s/it][A
 78%|███████▊  | 49/63 [00:49<00:14,  1.03s/it][A
 79%|███████▉  | 50/63 [00:50<00:13,  1.04s/it][A
 81%|████████  | 51/63 [00:51<00:12,  1.05s/it][A
 83%|████████▎ | 52/63 [00:52<00:11,  1.04s/it][A
 84%|████████▍ | 53/63 [00:53<00:11,  1.11s/it][A
 86%|████████▌ | 54/63 [00:54<00:09,  1.08s/it][A
 87%|████████▋ | 55/63 [00:55<00:08,  1.09s/it][A
 89%|████████▉ | 56/63 [00:56<00:07,  1.06s/it][A
 90%|█████████ | 57/63 [00:57<00:06,  1.07s/it][A
 92%|█████████▏| 58/63 [00:58<00:05,  1.05s/it][A
 94%|█████████▎| 59/63 [00:59<00:04,  1.06s/it][A
 95%|█████████▌| 60/63 [01:00<00:03,  1.05s/it][A
 97%|█████████▋| 61/63 [01:01<00:02,  1.07s/it][A
 98%|█████████▊| 62/63 [01:03<00:01,  1.05s/it][A
100%|██████████| 63/63 [01:04<00:00,  1.05s/it][A                                                   
                                               [A{'eval_loss': 0.37451171875, 'eval_runtime': 65.0549, 'eval_samples_per_second': 7.686, 'eval_steps_per_second': 0.968, 'epoch': 4.04}
 81%|████████  | 300/370 [4:44:40<47:08, 40.41s/it]
100%|██████████| 63/63 [01:04<00:00,  1.05s/it][A
                                               [A/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 81%|████████▏ | 301/370 [4:50:19<2:52:08, 149.68s/it]                                                      {'loss': 0.3751, 'learning_rate': 1e-05, 'epoch': 4.05}
 81%|████████▏ | 301/370 [4:50:19<2:52:08, 149.68s/it] 82%|████████▏ | 302/370 [4:51:01<2:13:05, 117.43s/it]                                                      {'loss': 0.3911, 'learning_rate': 1e-05, 'epoch': 4.07}
 82%|████████▏ | 302/370 [4:51:01<2:13:05, 117.43s/it] 82%|████████▏ | 303/370 [4:51:42<1:45:23, 94.39s/it]                                                      {'loss': 0.387, 'learning_rate': 1e-05, 'epoch': 4.08}
 82%|████████▏ | 303/370 [4:51:42<1:45:23, 94.39s/it] 82%|████████▏ | 304/370 [4:52:22<1:25:55, 78.12s/it]                                                     {'loss': 0.3628, 'learning_rate': 1e-05, 'epoch': 4.09}
 82%|████████▏ | 304/370 [4:52:22<1:25:55, 78.12s/it] 82%|████████▏ | 305/370 [4:53:03<1:12:23, 66.83s/it]                                                     {'loss': 0.3891, 'learning_rate': 1e-05, 'epoch': 4.11}
 82%|████████▏ | 305/370 [4:53:03<1:12:23, 66.83s/it] 83%|████████▎ | 306/370 [4:53:44<1:03:04, 59.13s/it]                                                     {'loss': 0.3717, 'learning_rate': 1e-05, 'epoch': 4.12}
 83%|████████▎ | 306/370 [4:53:44<1:03:04, 59.13s/it] 83%|████████▎ | 307/370 [4:54:24<56:12, 53.53s/it]                                                     {'loss': 0.373, 'learning_rate': 1e-05, 'epoch': 4.13}
 83%|████████▎ | 307/370 [4:54:24<56:12, 53.53s/it] 83%|████████▎ | 308/370 [4:55:04<51:08, 49.49s/it]                                                   {'loss': 0.3589, 'learning_rate': 1e-05, 'epoch': 4.15}
 83%|████████▎ | 308/370 [4:55:04<51:08, 49.49s/it] 84%|████████▎ | 309/370 [4:55:45<47:34, 46.79s/it]                                                   {'loss': 0.3774, 'learning_rate': 1e-05, 'epoch': 4.16}
 84%|████████▎ | 309/370 [4:55:45<47:34, 46.79s/it] 84%|████████▍ | 310/370 [4:56:25<44:48, 44.80s/it]                                                   {'loss': 0.3798, 'learning_rate': 1e-05, 'epoch': 4.18}
 84%|████████▍ | 310/370 [4:56:25<44:48, 44.80s/it] 84%|████████▍ | 311/370 [4:57:07<43:10, 43.91s/it]                                                   {'loss': 0.3793, 'learning_rate': 1e-05, 'epoch': 4.19}
 84%|████████▍ | 311/370 [4:57:07<43:10, 43.91s/it] 84%|████████▍ | 312/370 [4:57:47<41:23, 42.81s/it]                                                   {'loss': 0.3653, 'learning_rate': 1e-05, 'epoch': 4.2}
 84%|████████▍ | 312/370 [4:57:47<41:23, 42.81s/it] 85%|████████▍ | 313/370 [4:58:27<39:53, 42.00s/it]                                                   {'loss': 0.3728, 'learning_rate': 1e-05, 'epoch': 4.22}
 85%|████████▍ | 313/370 [4:58:27<39:53, 42.00s/it] 85%|████████▍ | 314/370 [4:59:08<38:54, 41.68s/it]                                                   {'loss': 0.3561, 'learning_rate': 1e-05, 'epoch': 4.23}
 85%|████████▍ | 314/370 [4:59:08<38:54, 41.68s/it] 85%|████████▌ | 315/370 [4:59:48<37:47, 41.23s/it]                                                   {'loss': 0.3656, 'learning_rate': 1e-05, 'epoch': 4.24}
 85%|████████▌ | 315/370 [4:59:48<37:47, 41.23s/it] 85%|████████▌ | 316/370 [5:00:30<37:09, 41.28s/it]                                                   {'loss': 0.3523, 'learning_rate': 1e-05, 'epoch': 4.26}
 85%|████████▌ | 316/370 [5:00:30<37:09, 41.28s/it] 86%|████████▌ | 317/370 [5:01:10<36:12, 40.98s/it]                                                   {'loss': 0.3577, 'learning_rate': 1e-05, 'epoch': 4.27}
 86%|████████▌ | 317/370 [5:01:10<36:12, 40.98s/it] 86%|████████▌ | 318/370 [5:01:50<35:09, 40.57s/it]                                                   {'loss': 0.3594, 'learning_rate': 1e-05, 'epoch': 4.28}
 86%|████████▌ | 318/370 [5:01:50<35:09, 40.57s/it] 86%|████████▌ | 319/370 [5:02:30<34:24, 40.47s/it]                                                   {'loss': 0.3598, 'learning_rate': 1e-05, 'epoch': 4.3}
 86%|████████▌ | 319/370 [5:02:30<34:24, 40.47s/it] 86%|████████▋ | 320/370 [5:03:10<33:37, 40.34s/it]                                                   {'loss': 0.3799, 'learning_rate': 1e-05, 'epoch': 4.31}
 86%|████████▋ | 320/370 [5:03:10<33:37, 40.34s/it] 87%|████████▋ | 321/370 [5:03:51<33:01, 40.45s/it]                                                   {'loss': 0.3614, 'learning_rate': 1e-05, 'epoch': 4.32}
 87%|████████▋ | 321/370 [5:03:51<33:01, 40.45s/it] 87%|████████▋ | 322/370 [5:04:31<32:17, 40.37s/it]                                                   {'loss': 0.3576, 'learning_rate': 1e-05, 'epoch': 4.34}
 87%|████████▋ | 322/370 [5:04:31<32:17, 40.37s/it] 87%|████████▋ | 323/370 [5:05:12<31:47, 40.58s/it]                                                   {'loss': 0.3812, 'learning_rate': 1e-05, 'epoch': 4.35}
 87%|████████▋ | 323/370 [5:05:12<31:47, 40.58s/it] 88%|████████▊ | 324/370 [5:05:52<31:07, 40.60s/it]                                                   {'loss': 0.3724, 'learning_rate': 1e-05, 'epoch': 4.36}
 88%|████████▊ | 324/370 [5:05:53<31:07, 40.60s/it] 88%|████████▊ | 325/370 [5:06:33<30:25, 40.56s/it]                                                   {'loss': 0.3582, 'learning_rate': 1e-05, 'epoch': 4.38}
 88%|████████▊ | 325/370 [5:06:33<30:25, 40.56s/it] 88%|████████▊ | 326/370 [5:07:14<29:44, 40.56s/it]                                                   {'loss': 0.383, 'learning_rate': 1e-05, 'epoch': 4.39}
 88%|████████▊ | 326/370 [5:07:14<29:44, 40.56s/it] 88%|████████▊ | 327/370 [5:07:54<29:00, 40.48s/it]                                                   {'loss': 0.348, 'learning_rate': 1e-05, 'epoch': 4.4}
 88%|████████▊ | 327/370 [5:07:54<29:00, 40.48s/it] 89%|████████▊ | 328/370 [5:08:34<28:20, 40.48s/it]                                                   {'loss': 0.388, 'learning_rate': 1e-05, 'epoch': 4.42}
 89%|████████▊ | 328/370 [5:08:34<28:20, 40.48s/it] 89%|████████▉ | 329/370 [5:09:15<27:39, 40.48s/it]                                                   {'loss': 0.3656, 'learning_rate': 1e-05, 'epoch': 4.43}
 89%|████████▉ | 329/370 [5:09:15<27:39, 40.48s/it] 89%|████████▉ | 330/370 [5:09:55<26:52, 40.31s/it]                                                   {'loss': 0.3806, 'learning_rate': 1e-05, 'epoch': 4.44}
 89%|████████▉ | 330/370 [5:09:55<26:52, 40.31s/it] 89%|████████▉ | 331/370 [5:10:35<26:10, 40.28s/it]                                                   {'loss': 0.3626, 'learning_rate': 1e-05, 'epoch': 4.46}
 89%|████████▉ | 331/370 [5:10:35<26:10, 40.28s/it] 90%|████████▉ | 332/370 [5:11:15<25:28, 40.23s/it]                                                   {'loss': 0.3553, 'learning_rate': 1e-05, 'epoch': 4.47}
 90%|████████▉ | 332/370 [5:11:15<25:28, 40.23s/it] 90%|█████████ | 333/370 [5:11:57<25:11, 40.85s/it]                                                   {'loss': 0.3737, 'learning_rate': 1e-05, 'epoch': 4.48}
 90%|█████████ | 333/370 [5:11:57<25:11, 40.85s/it] 90%|█████████ | 334/370 [5:12:39<24:35, 40.98s/it]                                                   {'loss': 0.3732, 'learning_rate': 1e-05, 'epoch': 4.5}
 90%|█████████ | 334/370 [5:12:39<24:35, 40.98s/it] 91%|█████████ | 335/370 [5:13:19<23:45, 40.73s/it]                                                   {'loss': 0.365, 'learning_rate': 1e-05, 'epoch': 4.51}
 91%|█████████ | 335/370 [5:13:19<23:45, 40.73s/it] 91%|█████████ | 336/370 [5:14:00<23:07, 40.82s/it]                                                   {'loss': 0.3543, 'learning_rate': 1e-05, 'epoch': 4.53}
 91%|█████████ | 336/370 [5:14:00<23:07, 40.82s/it] 91%|█████████ | 337/370 [5:14:40<22:19, 40.59s/it]                                                   {'loss': 0.3456, 'learning_rate': 1e-05, 'epoch': 4.54}
 91%|█████████ | 337/370 [5:14:40<22:19, 40.59s/it] 91%|█████████▏| 338/370 [5:15:20<21:36, 40.53s/it]                                                   {'loss': 0.3592, 'learning_rate': 1e-05, 'epoch': 4.55}
 91%|█████████▏| 338/370 [5:15:20<21:36, 40.53s/it] 92%|█████████▏| 339/370 [5:16:01<20:55, 40.50s/it]                                                   {'loss': 0.3373, 'learning_rate': 1e-05, 'epoch': 4.57}
 92%|█████████▏| 339/370 [5:16:01<20:55, 40.50s/it] 92%|█████████▏| 340/370 [5:16:41<20:13, 40.45s/it]                                                   {'loss': 0.367, 'learning_rate': 1e-05, 'epoch': 4.58}
 92%|█████████▏| 340/370 [5:16:41<20:13, 40.45s/it] 92%|█████████▏| 341/370 [5:17:20<19:24, 40.14s/it]                                                   {'loss': 0.3589, 'learning_rate': 1e-05, 'epoch': 4.59}
 92%|█████████▏| 341/370 [5:17:20<19:24, 40.14s/it] 92%|█████████▏| 342/370 [5:18:01<18:48, 40.30s/it]                                                   {'loss': 0.363, 'learning_rate': 1e-05, 'epoch': 4.61}
 92%|█████████▏| 342/370 [5:18:01<18:48, 40.30s/it] 93%|█████████▎| 343/370 [5:18:42<18:11, 40.43s/it]                                                   {'loss': 0.373, 'learning_rate': 1e-05, 'epoch': 4.62}
 93%|█████████▎| 343/370 [5:18:42<18:11, 40.43s/it] 93%|█████████▎| 344/370 [5:19:22<17:30, 40.40s/it]                                                   {'loss': 0.3724, 'learning_rate': 1e-05, 'epoch': 4.63}
 93%|█████████▎| 344/370 [5:19:22<17:30, 40.40s/it] 93%|█████████▎| 345/370 [5:20:03<16:51, 40.47s/it]                                                   {'loss': 0.3648, 'learning_rate': 1e-05, 'epoch': 4.65}
 93%|█████████▎| 345/370 [5:20:03<16:51, 40.47s/it] 94%|█████████▎| 346/370 [5:20:43<16:07, 40.30s/it]                                                   {'loss': 0.3533, 'learning_rate': 1e-05, 'epoch': 4.66}
 94%|█████████▎| 346/370 [5:20:43<16:07, 40.30s/it] 94%|█████████▍| 347/370 [5:21:23<15:26, 40.29s/it]                                                   {'loss': 0.3686, 'learning_rate': 1e-05, 'epoch': 4.67}
 94%|█████████▍| 347/370 [5:21:23<15:26, 40.29s/it] 94%|█████████▍| 348/370 [5:22:05<15:00, 40.93s/it]                                                   {'loss': 0.361, 'learning_rate': 1e-05, 'epoch': 4.69}
 94%|█████████▍| 348/370 [5:22:05<15:00, 40.93s/it] 94%|█████████▍| 349/370 [5:22:46<14:16, 40.76s/it]                                                   {'loss': 0.3516, 'learning_rate': 1e-05, 'epoch': 4.7}
 94%|█████████▍| 349/370 [5:22:46<14:16, 40.76s/it] 95%|█████████▍| 350/370 [5:23:27<13:38, 40.92s/it]                                                   {'loss': 0.3606, 'learning_rate': 1e-05, 'epoch': 4.71}
 95%|█████████▍| 350/370 [5:23:27<13:38, 40.92s/it] 95%|█████████▍| 351/370 [5:24:10<13:08, 41.49s/it]                                                   {'loss': 0.365, 'learning_rate': 1e-05, 'epoch': 4.73}
 95%|█████████▍| 351/370 [5:24:10<13:08, 41.49s/it] 95%|█████████▌| 352/370 [5:24:55<12:45, 42.52s/it]                                                   {'loss': 0.3337, 'learning_rate': 1e-05, 'epoch': 4.74}
 95%|█████████▌| 352/370 [5:24:55<12:45, 42.52s/it] 95%|█████████▌| 353/370 [5:25:45<12:43, 44.90s/it]                                                   {'loss': 0.3512, 'learning_rate': 1e-05, 'epoch': 4.75}
 95%|█████████▌| 353/370 [5:25:45<12:43, 44.90s/it] 96%|█████████▌| 354/370 [5:26:26<11:39, 43.71s/it]                                                   {'loss': 0.3567, 'learning_rate': 1e-05, 'epoch': 4.77}
 96%|█████████▌| 354/370 [5:26:26<11:39, 43.71s/it] 96%|█████████▌| 355/370 [5:27:06<10:40, 42.70s/it]                                                   {'loss': 0.3715, 'learning_rate': 1e-05, 'epoch': 4.78}
 96%|█████████▌| 355/370 [5:27:07<10:40, 42.70s/it] 96%|█████████▌| 356/370 [5:27:48<09:54, 42.46s/it]                                                   {'loss': 0.3703, 'learning_rate': 1e-05, 'epoch': 4.79}
 96%|█████████▌| 356/370 [5:27:48<09:54, 42.46s/it] 96%|█████████▋| 357/370 [5:28:29<09:04, 41.88s/it]                                                   {'loss': 0.3642, 'learning_rate': 1e-05, 'epoch': 4.81}
 96%|█████████▋| 357/370 [5:28:29<09:04, 41.88s/it] 97%|█████████▋| 358/370 [5:29:12<08:25, 42.10s/it]                                                   {'loss': 0.3564, 'learning_rate': 1e-05, 'epoch': 4.82}
 97%|█████████▋| 358/370 [5:29:12<08:25, 42.10s/it] 97%|█████████▋| 359/370 [5:29:53<07:42, 42.04s/it]                                                   {'loss': 0.3717, 'learning_rate': 1e-05, 'epoch': 4.84}
 97%|█████████▋| 359/370 [5:29:53<07:42, 42.04s/it] 97%|█████████▋| 360/370 [5:30:34<06:54, 41.49s/it]                                                   {'loss': 0.3757, 'learning_rate': 1e-05, 'epoch': 4.85}
 97%|█████████▋| 360/370 [5:30:34<06:54, 41.49s/it]
  0%|          | 0/63 [00:00<?, ?it/s][A
  3%|▎         | 2/63 [00:01<00:30,  1.99it/s][A
  5%|▍         | 3/63 [00:02<00:42,  1.40it/s][A
  6%|▋         | 4/63 [00:03<00:48,  1.22it/s][A
  8%|▊         | 5/63 [00:03<00:50,  1.14it/s][A
 10%|▉         | 6/63 [00:05<00:53,  1.06it/s][A
 11%|█         | 7/63 [00:06<00:54,  1.03it/s][A
 13%|█▎        | 8/63 [00:07<00:53,  1.02it/s][A
 14%|█▍        | 9/63 [00:08<00:53,  1.01it/s][A
 16%|█▌        | 10/63 [00:09<00:53,  1.00s/it][A
 17%|█▋        | 11/63 [00:10<00:53,  1.02s/it][A
 19%|█▉        | 12/63 [00:11<00:52,  1.03s/it][A
 21%|██        | 13/63 [00:12<00:52,  1.05s/it][A
 22%|██▏       | 14/63 [00:13<00:50,  1.03s/it][A
 24%|██▍       | 15/63 [00:14<00:50,  1.05s/it][A
 25%|██▌       | 16/63 [00:15<00:48,  1.04s/it][A
 27%|██▋       | 17/63 [00:16<00:48,  1.05s/it][A
 29%|██▊       | 18/63 [00:17<00:46,  1.04s/it][A
 30%|███       | 19/63 [00:18<00:45,  1.04s/it][A
 32%|███▏      | 20/63 [00:19<00:45,  1.05s/it][A
 33%|███▎      | 21/63 [00:20<00:44,  1.06s/it][A
 35%|███▍      | 22/63 [00:21<00:43,  1.05s/it][A
 37%|███▋      | 23/63 [00:22<00:42,  1.05s/it][A
 38%|███▊      | 24/63 [00:23<00:40,  1.03s/it][A
 40%|███▉      | 25/63 [00:24<00:38,  1.02s/it][A
 41%|████▏     | 26/63 [00:25<00:38,  1.05s/it][A
 43%|████▎     | 27/63 [00:26<00:37,  1.05s/it][A
 44%|████▍     | 28/63 [00:28<00:37,  1.07s/it][A
 46%|████▌     | 29/63 [00:29<00:35,  1.05s/it][A
 48%|████▊     | 30/63 [00:30<00:34,  1.04s/it][A
 49%|████▉     | 31/63 [00:31<00:32,  1.03s/it][A
 51%|█████     | 32/63 [00:32<00:31,  1.01s/it][A
 52%|█████▏    | 33/63 [00:33<00:30,  1.01s/it][A
 54%|█████▍    | 34/63 [00:34<00:29,  1.02s/it][A
 56%|█████▌    | 35/63 [00:35<00:28,  1.01s/it][A
 57%|█████▋    | 36/63 [00:36<00:27,  1.02s/it][A
 59%|█████▊    | 37/63 [00:37<00:27,  1.04s/it][A
 60%|██████    | 38/63 [00:38<00:26,  1.06s/it][A
 62%|██████▏   | 39/63 [00:39<00:25,  1.04s/it][A
 63%|██████▎   | 40/63 [00:40<00:23,  1.03s/it][A
 65%|██████▌   | 41/63 [00:41<00:23,  1.06s/it][A
 67%|██████▋   | 42/63 [00:42<00:22,  1.05s/it][A
 68%|██████▊   | 43/63 [00:43<00:21,  1.06s/it][A
 70%|██████▉   | 44/63 [00:44<00:19,  1.05s/it][A
 71%|███████▏  | 45/63 [00:45<00:18,  1.03s/it][A
 73%|███████▎  | 46/63 [00:46<00:17,  1.03s/it][A
 75%|███████▍  | 47/63 [00:47<00:16,  1.02s/it][A
 76%|███████▌  | 48/63 [00:48<00:15,  1.02s/it][A
 78%|███████▊  | 49/63 [00:49<00:14,  1.01s/it][A
 79%|███████▉  | 50/63 [00:50<00:13,  1.00s/it][A
 81%|████████  | 51/63 [00:51<00:11,  1.01it/s][A
 83%|████████▎ | 52/63 [00:52<00:11,  1.02s/it][A
 84%|████████▍ | 53/63 [00:53<00:10,  1.01s/it][A
 86%|████████▌ | 54/63 [00:54<00:09,  1.00s/it][A
 87%|████████▋ | 55/63 [00:55<00:08,  1.00s/it][A
 89%|████████▉ | 56/63 [00:56<00:07,  1.01s/it][A
 90%|█████████ | 57/63 [00:57<00:06,  1.02s/it][A
 92%|█████████▏| 58/63 [00:58<00:05,  1.05s/it][A
 94%|█████████▎| 59/63 [00:59<00:04,  1.05s/it][A
 95%|█████████▌| 60/63 [01:01<00:03,  1.03s/it][A
 97%|█████████▋| 61/63 [01:01<00:02,  1.06s/it][A
 98%|█████████▊| 62/63 [01:02<00:01,  1.03s/it][A
100%|██████████| 63/63 [01:03<00:00,  1.02s/it][A                                                   
                                               [A{'eval_loss': 0.361572265625, 'eval_runtime': 65.0263, 'eval_samples_per_second': 7.689, 'eval_steps_per_second': 0.969, 'epoch': 4.85}
 97%|█████████▋| 360/370 [5:31:39<06:54, 41.49s/it]
100%|██████████| 63/63 [01:03<00:00,  1.02s/it][A
                                               [A/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/jet/home/amartin1/.conda/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 98%|█████████▊| 361/370 [5:37:55<24:13, 161.48s/it]                                                    {'loss': 0.3636, 'learning_rate': 1e-05, 'epoch': 4.86}
 98%|█████████▊| 361/370 [5:37:55<24:13, 161.48s/it] 98%|█████████▊| 362/370 [5:38:36<16:42, 125.28s/it]                                                    {'loss': 0.3567, 'learning_rate': 1e-05, 'epoch': 4.88}
 98%|█████████▊| 362/370 [5:38:36<16:42, 125.28s/it] 98%|█████████▊| 363/370 [5:39:18<11:41, 100.28s/it]                                                    {'loss': 0.3484, 'learning_rate': 1e-05, 'epoch': 4.89}
 98%|█████████▊| 363/370 [5:39:18<11:41, 100.28s/it] 98%|█████████▊| 364/370 [5:39:58<08:14, 82.39s/it]                                                    {'loss': 0.3761, 'learning_rate': 1e-05, 'epoch': 4.9}
 98%|█████████▊| 364/370 [5:39:59<08:14, 82.39s/it] 99%|█████████▊| 365/370 [5:40:40<05:50, 70.17s/it]                                                   {'loss': 0.3618, 'learning_rate': 1e-05, 'epoch': 4.92}
 99%|█████████▊| 365/370 [5:40:40<05:50, 70.17s/it] 99%|█████████▉| 366/370 [5:41:21<04:05, 61.46s/it]                                                   {'loss': 0.3566, 'learning_rate': 1e-05, 'epoch': 4.93}
 99%|█████████▉| 366/370 [5:41:21<04:05, 61.46s/it] 99%|█████████▉| 367/370 [5:42:03<02:46, 55.40s/it]                                                   {'loss': 0.3625, 'learning_rate': 1e-05, 'epoch': 4.94}
 99%|█████████▉| 367/370 [5:42:03<02:46, 55.40s/it] 99%|█████████▉| 368/370 [5:42:43<01:41, 50.86s/it]                                                   {'loss': 0.352, 'learning_rate': 1e-05, 'epoch': 4.96}
 99%|█████████▉| 368/370 [5:42:43<01:41, 50.86s/it]100%|█████████▉| 369/370 [5:43:23<00:47, 47.68s/it]                                                   {'loss': 0.3447, 'learning_rate': 1e-05, 'epoch': 4.97}
100%|█████████▉| 369/370 [5:43:23<00:47, 47.68s/it]100%|██████████| 370/370 [5:44:48<00:00, 58.75s/it]                                                   {'loss': 0.3568, 'learning_rate': 1e-05, 'epoch': 4.98}
100%|██████████| 370/370 [5:44:48<00:00, 58.75s/it]                                                   {'train_runtime': 20693.9508, 'train_samples_per_second': 2.296, 'train_steps_per_second': 0.018, 'train_loss': 0.6006508234384897, 'epoch': 4.98}
100%|██████████| 370/370 [5:44:48<00:00, 58.75s/it]100%|██████████| 370/370 [5:44:48<00:00, 55.91s/it]
wandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.032 MB of 0.074 MB uploaded (0.001 MB deduped)wandb: | 0.063 MB of 0.075 MB uploaded (0.001 MB deduped)wandb: 
wandb: Run history:
wandb:                      eval/loss █▃▂▂▁▁
wandb:                   eval/runtime █▂▁▁▁▁
wandb:        eval/samples_per_second ▁▆▇███
wandb:          eval/steps_per_second ▁▆▇███
wandb:                    train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/learning_rate ▁▁██████████████████████████████████████
wandb:                     train/loss ███▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.36157
wandb:                   eval/runtime 65.0263
wandb:        eval/samples_per_second 7.689
wandb:          eval/steps_per_second 0.969
wandb:                    train/epoch 4.98
wandb:              train/global_step 370
wandb:            train/learning_rate 1e-05
wandb:                     train/loss 0.3568
wandb:               train/total_flos 5.323070464652739e+18
wandb:               train/train_loss 0.60065
wandb:            train/train_runtime 20693.9508
wandb: train/train_samples_per_second 2.296
wandb:   train/train_steps_per_second 0.018
wandb: 
wandb: 🚀 View run honest-haze-7 at: https://wandb.ai/athena-innovations/revl_k_0/runs/40fk06e4
wandb: ⭐️ View project at: https://wandb.ai/athena-innovations/revl_k_0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240918_210856-40fk06e4/logs
